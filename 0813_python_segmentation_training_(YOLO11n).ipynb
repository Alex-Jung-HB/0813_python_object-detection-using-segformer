{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5gPsLX2hjbEwt8UlkTDcs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Jung-HB/0813_python_object-detection-using-segformer-YOLO11n/blob/main/0813_python_segmentation_training_(YOLO11n).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO11 Object Detection Training"
      ],
      "metadata": {
        "id": "XZwsucxLXn3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOLO11 Training Tool - Fixed Dataset Handling\n",
        "\n",
        "For Jupyter/Colab users, use the simple function:\n",
        "    train_yolo_simple(\"/path/to/data.zip\", classes=\"all\", epochs=100)\n",
        "    train_yolo_simple(\"/path/to/data.zip\", classes=\"0,2,5\", epochs=50)\n",
        "\n",
        "For command line usage:\n",
        "    python yolo11_trainer.py --cli\n",
        "    python yolo11_trainer.py --zip data.zip --classes all --epochs 100\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import threading\n",
        "import shutil\n",
        "import argparse\n",
        "import subprocess\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Suppress common warnings that clutter the output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.*\")\n",
        "\n",
        "# These will be imported after checking if they're available\n",
        "# import torch\n",
        "# from ultralytics import YOLO\n",
        "\n",
        "# Try to import tkinter, handle if no display available\n",
        "GUI_AVAILABLE = True\n",
        "try:\n",
        "    import tkinter as tk\n",
        "    from tkinter import ttk, filedialog, messagebox, scrolledtext\n",
        "    # Test if display is available\n",
        "    root_test = tk.Tk()\n",
        "    root_test.withdraw()\n",
        "    root_test.destroy()\n",
        "except (ImportError, tk.TclError) as e:\n",
        "    GUI_AVAILABLE = False\n",
        "    print(\"=\" * 60)\n",
        "    print(\"YOLO11 Training Tool\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"GUI not available: {e}\")\n",
        "    print(\"Running in command-line mode...\")\n",
        "    print(\"\\nTo enable GUI:\")\n",
        "    print(\"- On Linux/WSL: Install X server (Xming, VcXsrv, or X11)\")\n",
        "    print(\"- On SSH: Use 'ssh -X' for X11 forwarding\")\n",
        "    print(\"- On headless servers: Use CLI mode with --cli flag\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "def check_and_install_packages():\n",
        "    \"\"\"Check and install required packages with better error handling\"\"\"\n",
        "    missing_packages = []\n",
        "    installation_commands = []\n",
        "\n",
        "    print(\"üîç Checking required packages...\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"‚úÖ PyTorch {torch.__version__} is available\")\n",
        "\n",
        "        # Check CUDA availability with better error handling\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_count = torch.cuda.device_count()\n",
        "            print(f\"‚úÖ CUDA available with {gpu_count} GPU(s)\")\n",
        "            for i in range(gpu_count):\n",
        "                try:\n",
        "                    gpu_name = torch.cuda.get_device_name(i)\n",
        "                    print(f\"   GPU {i}: {gpu_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   GPU {i}: Unknown (error: {e})\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  CUDA not available - will use CPU\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ùå PyTorch not found\")\n",
        "        missing_packages.append(\"torch\")\n",
        "        installation_commands.append(\"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\")\n",
        "\n",
        "    # Check Ultralytics\n",
        "    try:\n",
        "        import ultralytics\n",
        "        print(f\"‚úÖ Ultralytics {ultralytics.__version__} is available\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå Ultralytics not found\")\n",
        "        missing_packages.append(\"ultralytics\")\n",
        "        installation_commands.append(\"pip install ultralytics\")\n",
        "\n",
        "    # Install missing packages\n",
        "    if missing_packages:\n",
        "        print(f\"\\nüì¶ Installing {len(missing_packages)} missing package(s)...\")\n",
        "\n",
        "        for i, cmd in enumerate(installation_commands):\n",
        "            package_name = missing_packages[i]\n",
        "            print(f\"\\n‚è≥ Installing {package_name}...\")\n",
        "\n",
        "            try:\n",
        "                # Use subprocess with better error handling\n",
        "                result = subprocess.run(\n",
        "                    cmd.split(),\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=300  # 5 minute timeout\n",
        "                )\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    print(f\"‚úÖ {package_name} installed successfully\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to install {package_name}\")\n",
        "                    print(f\"Error: {result.stderr}\")\n",
        "                    return False\n",
        "\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(f\"‚ùå Installation of {package_name} timed out\")\n",
        "                return False\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error installing {package_name}: {e}\")\n",
        "                return False\n",
        "\n",
        "        print(\"\\nüîÑ Reloading modules...\")\n",
        "        # Try to import again after installation\n",
        "        try:\n",
        "            import torch\n",
        "            from ultralytics import YOLO\n",
        "            print(\"‚úÖ All packages loaded successfully\")\n",
        "        except ImportError as e:\n",
        "            print(f\"‚ùå Still missing packages after installation: {e}\")\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "class DatasetManager:\n",
        "    \"\"\"Handles all dataset operations with robust error handling\"\"\"\n",
        "\n",
        "    def __init__(self, log_func=print):\n",
        "        self.log = log_func\n",
        "        self.image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp'}\n",
        "\n",
        "    def analyze_dataset_structure(self, extract_path):\n",
        "        \"\"\"Analyze and report dataset structure with detailed logging\"\"\"\n",
        "        self.log(\"üîç Analyzing dataset structure...\")\n",
        "\n",
        "        structure_info = {\n",
        "            'images': [],\n",
        "            'labels': [],\n",
        "            'image_dirs': {},\n",
        "            'label_dirs': {},\n",
        "            'total_images': 0,\n",
        "            'total_labels': 0,\n",
        "            'class_ids': set()\n",
        "        }\n",
        "\n",
        "        # Walk through all directories\n",
        "        for root, dirs, files in os.walk(extract_path):\n",
        "            rel_root = os.path.relpath(root, extract_path)\n",
        "            if rel_root == '.':\n",
        "                rel_root = 'root'\n",
        "\n",
        "            # Count images and labels in this directory\n",
        "            images_in_dir = []\n",
        "            labels_in_dir = []\n",
        "\n",
        "            for file in files:\n",
        "                file_lower = file.lower()\n",
        "                if any(file_lower.endswith(ext) for ext in self.image_extensions):\n",
        "                    images_in_dir.append(file)\n",
        "                    structure_info['images'].append(os.path.join(root, file))\n",
        "                elif file_lower.endswith('.txt') and file_lower not in ['classes.txt', 'readme.txt']:\n",
        "                    labels_in_dir.append(file)\n",
        "                    structure_info['labels'].append(os.path.join(root, file))\n",
        "\n",
        "                    # Extract class IDs from this label file\n",
        "                    try:\n",
        "                        with open(os.path.join(root, file), 'r') as f:\n",
        "                            for line in f:\n",
        "                                if line.strip():\n",
        "                                    parts = line.split()\n",
        "                                    if len(parts) >= 5:\n",
        "                                        try:\n",
        "                                            class_id = int(parts[0])\n",
        "                                            structure_info['class_ids'].add(class_id)\n",
        "                                        except ValueError:\n",
        "                                            continue\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "            if images_in_dir:\n",
        "                structure_info['image_dirs'][rel_root] = len(images_in_dir)\n",
        "                structure_info['total_images'] += len(images_in_dir)\n",
        "\n",
        "            if labels_in_dir:\n",
        "                structure_info['label_dirs'][rel_root] = len(labels_in_dir)\n",
        "                structure_info['total_labels'] += len(labels_in_dir)\n",
        "\n",
        "        # Report findings\n",
        "        self.log(f\"üìä Dataset Analysis Complete:\")\n",
        "        self.log(f\"   üñºÔ∏è  Total images: {structure_info['total_images']}\")\n",
        "        self.log(f\"   üè∑Ô∏è  Total labels: {structure_info['total_labels']}\")\n",
        "        self.log(f\"   üéØ Unique classes: {len(structure_info['class_ids'])}\")\n",
        "\n",
        "        if structure_info['image_dirs']:\n",
        "            self.log(f\"   üìÅ Image directories:\")\n",
        "            for dir_name, count in structure_info['image_dirs'].items():\n",
        "                self.log(f\"      {dir_name}: {count} images\")\n",
        "\n",
        "        if structure_info['label_dirs']:\n",
        "            self.log(f\"   üìÇ Label directories:\")\n",
        "            for dir_name, count in structure_info['label_dirs'].items():\n",
        "                self.log(f\"      {dir_name}: {count} labels\")\n",
        "\n",
        "        return structure_info\n",
        "\n",
        "    def create_yolo_structure(self, base_path):\n",
        "        \"\"\"Create YOLO directory structure\"\"\"\n",
        "        yolo_dirs = {\n",
        "            'train_images': os.path.join(base_path, 'train', 'images'),\n",
        "            'train_labels': os.path.join(base_path, 'train', 'labels'),\n",
        "            'val_images': os.path.join(base_path, 'val', 'images'),\n",
        "            'val_labels': os.path.join(base_path, 'val', 'labels'),\n",
        "            'test_images': os.path.join(base_path, 'test', 'images'),\n",
        "            'test_labels': os.path.join(base_path, 'test', 'labels')\n",
        "        }\n",
        "\n",
        "        # Create all directories\n",
        "        for dir_name, dir_path in yolo_dirs.items():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "            self.log(f\"   üìÅ Created: {os.path.relpath(dir_path, base_path)}\")\n",
        "\n",
        "        return yolo_dirs\n",
        "\n",
        "    def find_image_label_pairs(self, structure_info):\n",
        "        \"\"\"Find matching image-label pairs\"\"\"\n",
        "        self.log(\"üîç Finding image-label pairs...\")\n",
        "\n",
        "        image_label_pairs = []\n",
        "        unmatched_images = []\n",
        "\n",
        "        for img_path in structure_info['images']:\n",
        "            img_name = os.path.basename(img_path)\n",
        "            img_name_no_ext = os.path.splitext(img_name)[0]\n",
        "\n",
        "            # Look for corresponding label file\n",
        "            label_path = None\n",
        "            for lbl_path in structure_info['labels']:\n",
        "                lbl_name = os.path.basename(lbl_path)\n",
        "                lbl_name_no_ext = os.path.splitext(lbl_name)[0]\n",
        "\n",
        "                if img_name_no_ext == lbl_name_no_ext:\n",
        "                    label_path = lbl_path\n",
        "                    break\n",
        "\n",
        "            if label_path and os.path.exists(label_path):\n",
        "                image_label_pairs.append((img_path, label_path, img_name))\n",
        "            else:\n",
        "                unmatched_images.append(img_name)\n",
        "\n",
        "        self.log(f\"‚úÖ Found {len(image_label_pairs)} valid image-label pairs\")\n",
        "        if unmatched_images:\n",
        "            self.log(f\"‚ö†Ô∏è  {len(unmatched_images)} images without labels\")\n",
        "            if len(unmatched_images) <= 10:\n",
        "                for img in unmatched_images[:10]:\n",
        "                    self.log(f\"      {img}\")\n",
        "            else:\n",
        "                for img in unmatched_images[:5]:\n",
        "                    self.log(f\"      {img}\")\n",
        "                self.log(f\"      ... and {len(unmatched_images) - 5} more\")\n",
        "\n",
        "        return image_label_pairs\n",
        "\n",
        "    def split_dataset(self, image_label_pairs, train_ratio=0.7, val_ratio=0.2):\n",
        "        \"\"\"Split dataset into train/val/test with minimum validation guarantee\"\"\"\n",
        "        if len(image_label_pairs) == 0:\n",
        "            return {'train': [], 'val': [], 'test': []}\n",
        "\n",
        "        # Shuffle for random split\n",
        "        random.shuffle(image_label_pairs)\n",
        "        total_pairs = len(image_label_pairs)\n",
        "\n",
        "        # Ensure minimum validation set size\n",
        "        min_val_size = max(1, min(10, total_pairs // 10))  # At least 1, max 10, or 10% of dataset\n",
        "\n",
        "        if total_pairs < 3:\n",
        "            # Very small dataset - put most in training, at least 1 in validation\n",
        "            if total_pairs == 1:\n",
        "                splits = {'train': image_label_pairs, 'val': [], 'test': []}\n",
        "            elif total_pairs == 2:\n",
        "                splits = {'train': image_label_pairs[:1], 'val': image_label_pairs[1:], 'test': []}\n",
        "            else:  # total_pairs == 3\n",
        "                splits = {'train': image_label_pairs[:2], 'val': image_label_pairs[2:], 'test': []}\n",
        "        else:\n",
        "            # Calculate split points\n",
        "            val_size = max(min_val_size, int(total_pairs * val_ratio))\n",
        "            test_size = max(1, int(total_pairs * (1 - train_ratio - val_ratio)))\n",
        "            train_size = total_pairs - val_size - test_size\n",
        "\n",
        "            # Ensure train_size is positive\n",
        "            if train_size < 1:\n",
        "                train_size = total_pairs - val_size\n",
        "                test_size = 0\n",
        "\n",
        "            train_end = train_size\n",
        "            val_end = train_size + val_size\n",
        "\n",
        "            splits = {\n",
        "                'train': image_label_pairs[:train_end],\n",
        "                'val': image_label_pairs[train_end:val_end],\n",
        "                'test': image_label_pairs[val_end:] if test_size > 0 else []\n",
        "            }\n",
        "\n",
        "        self.log(f\"üìä Dataset split:\")\n",
        "        self.log(f\"   üèãÔ∏è  Training: {len(splits['train'])} samples ({len(splits['train'])/total_pairs*100:.1f}%)\")\n",
        "        self.log(f\"   ‚úÖ Validation: {len(splits['val'])} samples ({len(splits['val'])/total_pairs*100:.1f}%)\")\n",
        "        if splits['test']:\n",
        "            self.log(f\"   üß™ Test: {len(splits['test'])} samples ({len(splits['test'])/total_pairs*100:.1f}%)\")\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def copy_files_to_splits(self, splits, yolo_dirs):\n",
        "        \"\"\"Copy files to train/val/test directories\"\"\"\n",
        "        self.log(\"üìã Copying files to YOLO structure...\")\n",
        "\n",
        "        total_copied = 0\n",
        "\n",
        "        for split_name, pairs in splits.items():\n",
        "            if len(pairs) == 0:\n",
        "                continue\n",
        "\n",
        "            img_dir = yolo_dirs[f'{split_name}_images']\n",
        "            lbl_dir = yolo_dirs[f'{split_name}_labels']\n",
        "\n",
        "            split_copied = 0\n",
        "\n",
        "            for img_path, lbl_path, img_name in pairs:\n",
        "                try:\n",
        "                    # Copy image\n",
        "                    if os.path.exists(img_path):\n",
        "                        shutil.copy2(img_path, img_dir)\n",
        "                        split_copied += 1\n",
        "                        total_copied += 1\n",
        "                    else:\n",
        "                        self.log(f\"‚ö†Ô∏è  Image not found: {img_path}\")\n",
        "                        continue\n",
        "\n",
        "                    # Copy label\n",
        "                    if lbl_path and os.path.exists(lbl_path):\n",
        "                        shutil.copy2(lbl_path, lbl_dir)\n",
        "                    else:\n",
        "                        self.log(f\"‚ö†Ô∏è  Label not found for: {img_name}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.log(f\"‚ùå Error copying {img_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            self.log(f\"   {split_name}: {split_copied} files copied\")\n",
        "\n",
        "        return total_copied > 0\n",
        "\n",
        "    def reorganize_dataset(self, extract_path, structure_info):\n",
        "        \"\"\"Complete dataset reorganization with robust error handling\"\"\"\n",
        "        self.log(\"üîÑ Reorganizing dataset to YOLO format...\")\n",
        "\n",
        "        # Create YOLO directory structure\n",
        "        yolo_dirs = self.create_yolo_structure(extract_path)\n",
        "\n",
        "        # Find image-label pairs\n",
        "        image_label_pairs = self.find_image_label_pairs(structure_info)\n",
        "\n",
        "        if len(image_label_pairs) == 0:\n",
        "            self.log(\"‚ùå No valid image-label pairs found!\")\n",
        "            return False\n",
        "\n",
        "        # Check if dataset already has splits\n",
        "        has_existing_splits = self.check_existing_splits(structure_info)\n",
        "\n",
        "        if has_existing_splits:\n",
        "            self.log(\"‚úÖ Preserving existing train/val splits\")\n",
        "            success = self.preserve_existing_splits(structure_info, yolo_dirs)\n",
        "        else:\n",
        "            self.log(\"üîÄ Creating new train/val/test splits\")\n",
        "            splits = self.split_dataset(image_label_pairs)\n",
        "            success = self.copy_files_to_splits(splits, yolo_dirs)\n",
        "\n",
        "        if not success:\n",
        "            return False\n",
        "\n",
        "        # Verify the reorganization\n",
        "        return self.verify_dataset_structure(yolo_dirs)\n",
        "\n",
        "    def check_existing_splits(self, structure_info):\n",
        "        \"\"\"Check if dataset already has train/val directory structure\"\"\"\n",
        "        has_train = any('train' in dir_name.lower() for dir_name in structure_info['image_dirs'].keys())\n",
        "        has_val = any('val' in dir_name.lower() or 'valid' in dir_name.lower() for dir_name in structure_info['image_dirs'].keys())\n",
        "        return has_train and has_val\n",
        "\n",
        "    def preserve_existing_splits(self, structure_info, yolo_dirs):\n",
        "        \"\"\"Preserve existing train/val/test splits\"\"\"\n",
        "        files_copied = 0\n",
        "\n",
        "        for img_path in structure_info['images']:\n",
        "            img_name = os.path.basename(img_path)\n",
        "            img_name_no_ext = os.path.splitext(img_name)[0]\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                continue\n",
        "\n",
        "            # Determine split based on directory path\n",
        "            dir_path = os.path.dirname(img_path).lower()\n",
        "\n",
        "            if 'train' in dir_path:\n",
        "                dest_img_dir = yolo_dirs['train_images']\n",
        "                dest_lbl_dir = yolo_dirs['train_labels']\n",
        "            elif 'val' in dir_path or 'valid' in dir_path:\n",
        "                dest_img_dir = yolo_dirs['val_images']\n",
        "                dest_lbl_dir = yolo_dirs['val_labels']\n",
        "            elif 'test' in dir_path:\n",
        "                dest_img_dir = yolo_dirs['test_images']\n",
        "                dest_lbl_dir = yolo_dirs['test_labels']\n",
        "            else:\n",
        "                # Default to train if unclear\n",
        "                dest_img_dir = yolo_dirs['train_images']\n",
        "                dest_lbl_dir = yolo_dirs['train_labels']\n",
        "\n",
        "            try:\n",
        "                # Copy image\n",
        "                shutil.copy2(img_path, dest_img_dir)\n",
        "                files_copied += 1\n",
        "\n",
        "                # Find and copy corresponding label\n",
        "                for lbl_path in structure_info['labels']:\n",
        "                    lbl_name = os.path.basename(lbl_path)\n",
        "                    lbl_name_no_ext = os.path.splitext(lbl_name)[0]\n",
        "\n",
        "                    if img_name_no_ext == lbl_name_no_ext:\n",
        "                        if os.path.exists(lbl_path):\n",
        "                            shutil.copy2(lbl_path, dest_lbl_dir)\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                self.log(f\"‚ùå Error copying {img_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        self.log(f\"‚úÖ Copied {files_copied} files preserving splits\")\n",
        "        return files_copied > 0\n",
        "\n",
        "    def verify_dataset_structure(self, yolo_dirs):\n",
        "        \"\"\"Verify that the dataset structure is correct\"\"\"\n",
        "        self.log(\"üîç Verifying dataset structure...\")\n",
        "\n",
        "        required_dirs = ['train_images', 'train_labels', 'val_images', 'val_labels']\n",
        "\n",
        "        for dir_name in required_dirs:\n",
        "            dir_path = yolo_dirs[dir_name]\n",
        "\n",
        "            if not os.path.exists(dir_path):\n",
        "                self.log(f\"‚ùå Missing directory: {dir_path}\")\n",
        "                return False\n",
        "\n",
        "            # Count files\n",
        "            if 'images' in dir_name:\n",
        "                files = [f for f in os.listdir(dir_path) if f.lower().endswith(tuple(self.image_extensions))]\n",
        "            else:\n",
        "                files = [f for f in os.listdir(dir_path) if f.endswith('.txt')]\n",
        "\n",
        "            file_count = len(files)\n",
        "            self.log(f\"   ‚úÖ {dir_name}: {file_count} files\")\n",
        "\n",
        "            # Check for empty critical directories\n",
        "            if file_count == 0:\n",
        "                if dir_name in ['train_images', 'train_labels']:\n",
        "                    self.log(f\"‚ùå Critical directory is empty: {dir_name}\")\n",
        "                    return False\n",
        "                elif dir_name in ['val_images', 'val_labels']:\n",
        "                    self.log(f\"‚ö†Ô∏è  Validation directory is empty: {dir_name}\")\n",
        "                    # Try to create validation set from training data\n",
        "                    return self.create_validation_from_training(yolo_dirs)\n",
        "\n",
        "        self.log(\"‚úÖ Dataset structure verification passed\")\n",
        "        return True\n",
        "\n",
        "    def create_validation_from_training(self, yolo_dirs):\n",
        "        \"\"\"Create validation set from training data when validation is empty\"\"\"\n",
        "        self.log(\"üîÑ Creating validation set from training data...\")\n",
        "\n",
        "        train_images_dir = yolo_dirs['train_images']\n",
        "        train_labels_dir = yolo_dirs['train_labels']\n",
        "        val_images_dir = yolo_dirs['val_images']\n",
        "        val_labels_dir = yolo_dirs['val_labels']\n",
        "\n",
        "        # Get all training images\n",
        "        train_images = [f for f in os.listdir(train_images_dir) if f.lower().endswith(tuple(self.image_extensions))]\n",
        "\n",
        "        if len(train_images) < 2:\n",
        "            self.log(\"‚ùå Not enough training images to create validation set\")\n",
        "            return False\n",
        "\n",
        "        # Move 20% of training to validation (minimum 1, maximum 20)\n",
        "        val_count = max(1, min(20, len(train_images) // 5))\n",
        "\n",
        "        # Randomly select images for validation\n",
        "        random.shuffle(train_images)\n",
        "        val_images = train_images[:val_count]\n",
        "\n",
        "        self.log(f\"üì¶ Moving {len(val_images)} samples to validation...\")\n",
        "\n",
        "        moved_count = 0\n",
        "        for img_file in val_images:\n",
        "            img_name_no_ext = os.path.splitext(img_file)[0]\n",
        "\n",
        "            # Move image\n",
        "            src_img = os.path.join(train_images_dir, img_file)\n",
        "            dst_img = os.path.join(val_images_dir, img_file)\n",
        "\n",
        "            if os.path.exists(src_img):\n",
        "                shutil.move(src_img, dst_img)\n",
        "                moved_count += 1\n",
        "\n",
        "                # Move corresponding label if exists\n",
        "                label_file = img_name_no_ext + '.txt'\n",
        "                src_label = os.path.join(train_labels_dir, label_file)\n",
        "                dst_label = os.path.join(val_labels_dir, label_file)\n",
        "\n",
        "                if os.path.exists(src_label):\n",
        "                    shutil.move(src_label, dst_label)\n",
        "\n",
        "        self.log(f\"‚úÖ Created validation set with {moved_count} samples\")\n",
        "        return moved_count > 0\n",
        "\n",
        "\n",
        "class YOLO11Trainer:\n",
        "    \"\"\"GUI interface for YOLO11 training with improved dataset handling\"\"\"\n",
        "\n",
        "    def __init__(self, root):\n",
        "        if not GUI_AVAILABLE:\n",
        "            raise RuntimeError(\"GUI not available - no display found\")\n",
        "\n",
        "        self.root = root\n",
        "        self.root.title(\"YOLO11 Training Tool\")\n",
        "        self.root.geometry(\"900x700\")\n",
        "\n",
        "        # Initialize dataset manager\n",
        "        self.dataset_manager = DatasetManager(self.log_message)\n",
        "\n",
        "        # Variables\n",
        "        self.data_path = tk.StringVar()\n",
        "        self.selected_classes = []\n",
        "        self.all_classes = []\n",
        "        self.device_info = tk.StringVar()\n",
        "\n",
        "        # Check GPU availability on startup\n",
        "        self.check_gpu()\n",
        "\n",
        "        # Create GUI\n",
        "        self.create_widgets()\n",
        "\n",
        "    def check_gpu(self):\n",
        "        \"\"\"Check for available GPUs and set up environment\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "        except ImportError:\n",
        "            self.device_info.set(\"‚ö†Ô∏è PyTorch not available\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_count = torch.cuda.device_count()\n",
        "                gpu_names = []\n",
        "                for i in range(gpu_count):\n",
        "                    try:\n",
        "                        gpu_names.append(torch.cuda.get_device_name(i))\n",
        "                    except Exception:\n",
        "                        gpu_names.append(f\"GPU {i}\")\n",
        "\n",
        "                device_text = f\"‚úÖ {gpu_count} GPU(s): {', '.join(gpu_names[:2])}\"\n",
        "                if len(gpu_names) > 2:\n",
        "                    device_text += f\" (+{len(gpu_names)-2} more)\"\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, range(gpu_count)))\n",
        "            else:\n",
        "                device_text = \"‚ö†Ô∏è No GPU available, will use CPU\"\n",
        "        except Exception as e:\n",
        "            device_text = f\"‚ö†Ô∏è GPU check failed: {str(e)[:50]}...\"\n",
        "\n",
        "        self.device_info.set(device_text)\n",
        "\n",
        "    def create_widgets(self):\n",
        "        # Main frame with scrollable content\n",
        "        main_frame = ttk.Frame(self.root, padding=\"10\")\n",
        "        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
        "\n",
        "        # GPU info\n",
        "        ttk.Label(main_frame, text=\"Device Status:\").grid(row=0, column=0, sticky=tk.W, pady=5)\n",
        "        ttk.Label(main_frame, textvariable=self.device_info, wraplength=600).grid(row=0, column=1, columnspan=2, sticky=tk.W, pady=5)\n",
        "\n",
        "        # File selection\n",
        "        ttk.Label(main_frame, text=\"Training Data (ZIP):\").grid(row=1, column=0, sticky=tk.W, pady=5)\n",
        "        ttk.Entry(main_frame, textvariable=self.data_path, width=60).grid(row=1, column=1, sticky=(tk.W, tk.E), pady=5)\n",
        "        ttk.Button(main_frame, text=\"Browse\", command=self.select_zip_file).grid(row=1, column=2, pady=5, padx=(5,0))\n",
        "\n",
        "        # Load data button\n",
        "        ttk.Button(main_frame, text=\"üîç Load & Analyze Data\", command=self.load_data).grid(row=2, column=0, columnspan=3, pady=10)\n",
        "\n",
        "        # Classes selection frame\n",
        "        classes_frame = ttk.LabelFrame(main_frame, text=\"Select Objects to Train\", padding=\"10\")\n",
        "        classes_frame.grid(row=3, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)\n",
        "\n",
        "        # Classes listbox with scrollbar\n",
        "        list_frame = ttk.Frame(classes_frame)\n",
        "        list_frame.grid(row=0, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
        "\n",
        "        self.classes_listbox = tk.Listbox(list_frame, selectmode=tk.MULTIPLE, height=8)\n",
        "        scrollbar_classes = ttk.Scrollbar(list_frame, orient=tk.VERTICAL, command=self.classes_listbox.yview)\n",
        "        self.classes_listbox.configure(yscrollcommand=scrollbar_classes.set)\n",
        "\n",
        "        self.classes_listbox.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
        "        scrollbar_classes.grid(row=0, column=1, sticky=(tk.N, tk.S))\n",
        "\n",
        "        # Buttons for class selection\n",
        "        ttk.Button(classes_frame, text=\"Select All\", command=self.select_all_classes).grid(row=1, column=0, pady=5, padx=(0,5))\n",
        "        ttk.Button(classes_frame, text=\"Clear Selection\", command=self.clear_selection).grid(row=1, column=1, pady=5, padx=5)\n",
        "\n",
        "        # Training parameters\n",
        "        params_frame = ttk.LabelFrame(main_frame, text=\"Training Parameters\", padding=\"10\")\n",
        "        params_frame.grid(row=4, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=5)\n",
        "\n",
        "        # Row 0: Epochs and Image Size\n",
        "        ttk.Label(params_frame, text=\"Epochs:\").grid(row=0, column=0, sticky=tk.W)\n",
        "        self.epochs_var = tk.StringVar(value=\"100\")\n",
        "        ttk.Entry(params_frame, textvariable=self.epochs_var, width=10).grid(row=0, column=1, sticky=tk.W, padx=5)\n",
        "\n",
        "        ttk.Label(params_frame, text=\"Image Size:\").grid(row=0, column=2, sticky=tk.W, padx=(20,0))\n",
        "        self.imgsz_var = tk.StringVar(value=\"640\")\n",
        "        ttk.Entry(params_frame, textvariable=self.imgsz_var, width=10).grid(row=0, column=3, sticky=tk.W, padx=5)\n",
        "\n",
        "        # Row 1: Batch Size and Model\n",
        "        ttk.Label(params_frame, text=\"Batch Size:\").grid(row=1, column=0, sticky=tk.W)\n",
        "        self.batch_var = tk.StringVar(value=\"16\")\n",
        "        ttk.Entry(params_frame, textvariable=self.batch_var, width=10).grid(row=1, column=1, sticky=tk.W, padx=5)\n",
        "\n",
        "        ttk.Label(params_frame, text=\"Model:\").grid(row=1, column=2, sticky=tk.W, padx=(20,0))\n",
        "        self.model_var = tk.StringVar(value=\"yolo11n.pt\")\n",
        "        model_combo = ttk.Combobox(params_frame, textvariable=self.model_var, width=12, state=\"readonly\")\n",
        "        model_combo['values'] = (\"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\")\n",
        "        model_combo.grid(row=1, column=3, sticky=tk.W, padx=5)\n",
        "\n",
        "        # Train button\n",
        "        self.train_button = ttk.Button(main_frame, text=\"üöÄ Start Training\", command=self.start_training, state=tk.DISABLED)\n",
        "        self.train_button.grid(row=5, column=0, columnspan=3, pady=20)\n",
        "\n",
        "        # Progress and log\n",
        "        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')\n",
        "        self.progress.grid(row=6, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=5)\n",
        "\n",
        "        # Log text area\n",
        "        log_frame = ttk.LabelFrame(main_frame, text=\"Training Log\", padding=\"5\")\n",
        "        log_frame.grid(row=7, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)\n",
        "\n",
        "        self.log_text = scrolledtext.ScrolledText(log_frame, height=12, width=80)\n",
        "        self.log_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
        "\n",
        "        # Configure grid weights for responsiveness\n",
        "        self.root.columnconfigure(0, weight=1)\n",
        "        self.root.rowconfigure(0, weight=1)\n",
        "        main_frame.columnconfigure(1, weight=1)\n",
        "        main_frame.rowconfigure(3, weight=1)\n",
        "        main_frame.rowconfigure(7, weight=1)\n",
        "        classes_frame.columnconfigure(0, weight=1)\n",
        "        classes_frame.rowconfigure(0, weight=1)\n",
        "        list_frame.columnconfigure(0, weight=1)\n",
        "        list_frame.rowconfigure(0, weight=1)\n",
        "        log_frame.columnconfigure(0, weight=1)\n",
        "        log_frame.rowconfigure(0, weight=1)\n",
        "\n",
        "    def log_message(self, message):\n",
        "        \"\"\"Add message to log area with timestamp\"\"\"\n",
        "        timestamp = time.strftime(\"%H:%M:%S\")\n",
        "        self.log_text.insert(tk.END, f\"[{timestamp}] {message}\\n\")\n",
        "        self.log_text.see(tk.END)\n",
        "        self.root.update_idletasks()\n",
        "\n",
        "    def select_zip_file(self):\n",
        "        \"\"\"Open file dialog to select zip file\"\"\"\n",
        "        file_path = filedialog.askopenfilename(\n",
        "            title=\"Select Training Data ZIP File\",\n",
        "            filetypes=[(\"ZIP files\", \"*.zip\"), (\"All files\", \"*.*\")]\n",
        "        )\n",
        "        if file_path:\n",
        "            self.data_path.set(file_path)\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and analyze the ZIP file\"\"\"\n",
        "        if not self.data_path.get():\n",
        "            messagebox.showerror(\"Error\", \"Please select a ZIP file first\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.log_message(\"üöÄ Starting data loading process...\")\n",
        "\n",
        "            # Extract ZIP file\n",
        "            extract_path = \"./temp_data\"\n",
        "            if os.path.exists(extract_path):\n",
        "                shutil.rmtree(extract_path)\n",
        "            os.makedirs(extract_path)\n",
        "\n",
        "            self.log_message(\"üì¶ Extracting ZIP file...\")\n",
        "            with zipfile.ZipFile(self.data_path.get(), 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_path)\n",
        "\n",
        "            self.log_message(\"‚úÖ ZIP file extracted successfully\")\n",
        "\n",
        "            # Analyze dataset structure\n",
        "            structure_info = self.dataset_manager.analyze_dataset_structure(extract_path)\n",
        "\n",
        "            if structure_info['total_images'] == 0:\n",
        "                messagebox.showerror(\"Error\", \"No image files found in the dataset\")\n",
        "                return\n",
        "\n",
        "            if structure_info['total_labels'] == 0:\n",
        "                messagebox.showerror(\"Error\", \"No label files (.txt) found in the dataset\")\n",
        "                return\n",
        "\n",
        "            # Reorganize dataset to YOLO format\n",
        "            if not self.dataset_manager.reorganize_dataset(extract_path, structure_info):\n",
        "                messagebox.showerror(\"Error\", \"Failed to reorganize dataset properly\")\n",
        "                return\n",
        "\n",
        "            # Extract class information\n",
        "            all_class_ids = sorted(list(structure_info['class_ids']))\n",
        "            if not all_class_ids:\n",
        "                messagebox.showerror(\"Error\", \"No valid class labels found in dataset\")\n",
        "                return\n",
        "\n",
        "            # Load class names\n",
        "            self.all_classes = self.load_class_names(extract_path, all_class_ids)\n",
        "\n",
        "            # Populate listbox\n",
        "            self.classes_listbox.delete(0, tk.END)\n",
        "            for i, class_name in enumerate(self.all_classes):\n",
        "                self.classes_listbox.insert(tk.END, f\"{all_class_ids[i]}: {class_name}\")\n",
        "\n",
        "            self.log_message(f\"‚úÖ Found {len(self.all_classes)} object classes\")\n",
        "            self.train_button.config(state=tk.NORMAL)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Failed to load data: {str(e)}\"\n",
        "            self.log_message(f\"‚ùå {error_msg}\")\n",
        "            messagebox.showerror(\"Error\", error_msg)\n",
        "\n",
        "    def load_class_names(self, extract_path, all_class_ids):\n",
        "        \"\"\"Load class names from various sources\"\"\"\n",
        "        # Start with default names\n",
        "        class_names = [f\"Class_{i}\" for i in all_class_ids]\n",
        "\n",
        "        # Try to load from classes.txt or YAML files\n",
        "        for root, dirs, files in os.walk(extract_path):\n",
        "            # Check for classes.txt\n",
        "            if 'classes.txt' in files:\n",
        "                try:\n",
        "                    with open(os.path.join(root, 'classes.txt'), 'r') as f:\n",
        "                        names = [line.strip() for line in f if line.strip()]\n",
        "                        if len(names) >= max(all_class_ids) + 1:\n",
        "                            class_names = [names[i] for i in all_class_ids]\n",
        "                            self.log_message(\"üìù Loaded class names from classes.txt\")\n",
        "                            return class_names\n",
        "                except Exception as e:\n",
        "                    self.log_message(f\"‚ö†Ô∏è Error reading classes.txt: {e}\")\n",
        "\n",
        "            # Check for YAML files\n",
        "            for file in files:\n",
        "                if file.endswith(('.yaml', '.yml')):\n",
        "                    try:\n",
        "                        with open(os.path.join(root, file), 'r') as f:\n",
        "                            data = yaml.safe_load(f)\n",
        "                            if 'names' in data:\n",
        "                                names = data['names']\n",
        "                                if isinstance(names, list) and len(names) >= max(all_class_ids) + 1:\n",
        "                                    class_names = [names[i] for i in all_class_ids]\n",
        "                                    self.log_message(f\"üìù Loaded class names from {file}\")\n",
        "                                    return class_names\n",
        "                                elif isinstance(names, dict):\n",
        "                                    class_names = [names.get(i, f\"Class_{i}\") for i in all_class_ids]\n",
        "                                    self.log_message(f\"üìù Loaded class names from {file}\")\n",
        "                                    return class_names\n",
        "                    except Exception as e:\n",
        "                        self.log_message(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n",
        "\n",
        "        self.log_message(\"üìù Using default class names\")\n",
        "        return class_names\n",
        "\n",
        "    def select_all_classes(self):\n",
        "        \"\"\"Select all classes in the listbox\"\"\"\n",
        "        self.classes_listbox.select_set(0, tk.END)\n",
        "\n",
        "    def clear_selection(self):\n",
        "        \"\"\"Clear all selections in the listbox\"\"\"\n",
        "        self.classes_listbox.selection_clear(0, tk.END)\n",
        "\n",
        "    def start_training(self):\n",
        "        \"\"\"Start YOLO training in a separate thread\"\"\"\n",
        "        selected_indices = [int(self.classes_listbox.get(i).split(':')[0])\n",
        "                          for i in self.classes_listbox.curselection()]\n",
        "\n",
        "        if not selected_indices:\n",
        "            messagebox.showerror(\"Error\", \"Please select at least one class to train\")\n",
        "            return\n",
        "\n",
        "        # Disable training button and start progress\n",
        "        self.train_button.config(state=tk.DISABLED)\n",
        "        self.progress.start()\n",
        "\n",
        "        # Start training in separate thread\n",
        "        training_thread = threading.Thread(target=self.train_model, args=(selected_indices,))\n",
        "        training_thread.daemon = True\n",
        "        training_thread.start()\n",
        "\n",
        "    def train_model(self, selected_indices):\n",
        "        \"\"\"Train YOLO model with improved error handling\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "            from ultralytics import YOLO\n",
        "        except ImportError as e:\n",
        "            error_msg = f\"Required packages not available: {e}\"\n",
        "            self.log_message(f\"‚ùå {error_msg}\")\n",
        "            self.root.after(0, lambda: [\n",
        "                self.train_button.config(state=tk.NORMAL),\n",
        "                self.progress.stop(),\n",
        "                messagebox.showerror(\"Error\", error_msg)\n",
        "            ])\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.log_message(f\"üöÄ Starting training with {len(selected_indices)} classes...\")\n",
        "\n",
        "            # Create dataset configuration\n",
        "            class_mapping = self.create_dataset_yaml(selected_indices)\n",
        "\n",
        "            # Filter labels to only include selected classes\n",
        "            self.filter_labels(class_mapping)\n",
        "\n",
        "            # Final verification\n",
        "            if not self.final_dataset_check():\n",
        "                error_msg = \"Dataset verification failed\"\n",
        "                self.log_message(f\"‚ùå {error_msg}\")\n",
        "                self.root.after(0, lambda: [\n",
        "                    self.train_button.config(state=tk.NORMAL),\n",
        "                    self.progress.stop(),\n",
        "                    messagebox.showerror(\"Error\", error_msg)\n",
        "                ])\n",
        "                return\n",
        "\n",
        "            # Initialize YOLO model\n",
        "            model_name = self.model_var.get()\n",
        "            self.log_message(f\"ü§ñ Loading {model_name} model...\")\n",
        "            model = YOLO(model_name)\n",
        "\n",
        "            # Training parameters\n",
        "            epochs = int(self.epochs_var.get())\n",
        "            imgsz = int(self.imgsz_var.get())\n",
        "            batch = int(self.batch_var.get())\n",
        "\n",
        "            # Adjust batch size for GPU memory\n",
        "            if torch.cuda.is_available():\n",
        "                try:\n",
        "                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                    if gpu_memory < 8 and batch > 8:\n",
        "                        batch = 8\n",
        "                        self.log_message(f\"üìâ Reduced batch size to {batch} for GPU memory\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            # Start training\n",
        "            self.log_message(\"üèÉ Training started...\")\n",
        "            self.log_message(f\"Parameters: epochs={epochs}, imgsz={imgsz}, batch={batch}\")\n",
        "\n",
        "            yaml_path = os.path.abspath('./dataset.yaml')\n",
        "            self.log_message(f\"üìÑ Using dataset config: {yaml_path}\")\n",
        "\n",
        "            results = model.train(\n",
        "                data=yaml_path,\n",
        "                epochs=epochs,\n",
        "                imgsz=imgsz,\n",
        "                batch=batch,\n",
        "                device='0' if torch.cuda.is_available() else 'cpu',\n",
        "                project='./runs/train',\n",
        "                name='yolo11_custom',\n",
        "                exist_ok=True,\n",
        "                verbose=True,\n",
        "                patience=20,\n",
        "                save_period=max(10, epochs // 10)\n",
        "            )\n",
        "\n",
        "            self.log_message(\"üéâ Training completed successfully!\")\n",
        "            self.log_message(f\"üìÅ Model saved to: {results.save_dir}\")\n",
        "\n",
        "            # Re-enable training button\n",
        "            self.root.after(0, lambda: [\n",
        "                self.train_button.config(state=tk.NORMAL),\n",
        "                self.progress.stop(),\n",
        "                messagebox.showinfo(\"Success\", f\"Training completed!\\nModel saved to: {results.save_dir}\")\n",
        "            ])\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Training failed: {str(e)}\"\n",
        "            self.log_message(f\"‚ùå {error_msg}\")\n",
        "            self.log_message(\"üí° Troubleshooting tips:\")\n",
        "            self.log_message(\"‚Ä¢ Check dataset paths and file permissions\")\n",
        "            self.log_message(\"‚Ä¢ Try reducing batch size or image size\")\n",
        "            self.log_message(\"‚Ä¢ Verify all images and labels are valid\")\n",
        "\n",
        "            self.root.after(0, lambda: [\n",
        "                self.train_button.config(state=tk.NORMAL),\n",
        "                self.progress.stop(),\n",
        "                messagebox.showerror(\"Error\", error_msg)\n",
        "            ])\n",
        "\n",
        "    def create_dataset_yaml(self, selected_indices):\n",
        "        \"\"\"Create dataset.yaml file for YOLO training\"\"\"\n",
        "        class_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(selected_indices)}\n",
        "        selected_names = [self.all_classes[i] for i in selected_indices]\n",
        "\n",
        "        temp_data_path = os.path.abspath('./temp_data')\n",
        "\n",
        "        dataset_config = {\n",
        "            'path': temp_data_path,\n",
        "            'train': 'train/images',\n",
        "            'val': 'val/images',\n",
        "            'test': 'test/images',\n",
        "            'nc': len(selected_indices),\n",
        "            'names': selected_names\n",
        "        }\n",
        "\n",
        "        yaml_path = './dataset.yaml'\n",
        "        with open(yaml_path, 'w') as f:\n",
        "            yaml.dump(dataset_config, f, default_flow_style=False)\n",
        "\n",
        "        self.log_message(f\"üìÑ Created dataset.yaml with {len(selected_indices)} classes\")\n",
        "\n",
        "        return class_mapping\n",
        "\n",
        "    def filter_labels(self, class_mapping):\n",
        "        \"\"\"Filter label files to only include selected classes\"\"\"\n",
        "        self.log_message(\"üîÑ Filtering labels for selected classes...\")\n",
        "\n",
        "        label_dirs = [\n",
        "            './temp_data/train/labels',\n",
        "            './temp_data/val/labels',\n",
        "            './temp_data/test/labels'\n",
        "        ]\n",
        "\n",
        "        total_filtered = 0\n",
        "\n",
        "        for label_dir in label_dirs:\n",
        "            if os.path.exists(label_dir):\n",
        "                label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]\n",
        "\n",
        "                for file in label_files:\n",
        "                    label_path = os.path.join(label_dir, file)\n",
        "                    if self.filter_label_file(label_path, class_mapping):\n",
        "                        total_filtered += 1\n",
        "\n",
        "        self.log_message(f\"‚úÖ Filtered {total_filtered} label files\")\n",
        "\n",
        "    def filter_label_file(self, label_path, class_mapping):\n",
        "        \"\"\"Filter individual label file\"\"\"\n",
        "        try:\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            filtered_lines = []\n",
        "\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 5:\n",
        "                        try:\n",
        "                            class_id = int(parts[0])\n",
        "                            if class_id in class_mapping:\n",
        "                                parts[0] = str(class_mapping[class_id])\n",
        "                                filtered_lines.append(' '.join(parts) + '\\n')\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "            with open(label_path, 'w') as f:\n",
        "                f.writelines(filtered_lines)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_message(f\"‚ö†Ô∏è Error filtering {label_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def final_dataset_check(self):\n",
        "        \"\"\"Final check before training starts\"\"\"\n",
        "        self.log_message(\"üîç Final dataset verification...\")\n",
        "\n",
        "        # Check required paths\n",
        "        required_paths = [\n",
        "            './temp_data/train/images',\n",
        "            './temp_data/train/labels',\n",
        "            './temp_data/val/images',\n",
        "            './temp_data/val/labels',\n",
        "            './dataset.yaml'\n",
        "        ]\n",
        "\n",
        "        for path in required_paths:\n",
        "            if not os.path.exists(path):\n",
        "                self.log_message(f\"‚ùå Missing: {path}\")\n",
        "                return False\n",
        "\n",
        "        # Count files\n",
        "        train_images = len([f for f in os.listdir('./temp_data/train/images')\n",
        "                           if f.lower().endswith(tuple(self.dataset_manager.image_extensions))])\n",
        "        val_images = len([f for f in os.listdir('./temp_data/val/images')\n",
        "                         if f.lower().endswith(tuple(self.dataset_manager.image_extensions))])\n",
        "\n",
        "        if train_images == 0:\n",
        "            self.log_message(\"‚ùå No training images found\")\n",
        "            return False\n",
        "\n",
        "        if val_images == 0:\n",
        "            self.log_message(\"‚ùå No validation images found\")\n",
        "            return False\n",
        "\n",
        "        self.log_message(f\"‚úÖ Final check passed: {train_images} train, {val_images} val images\")\n",
        "        return True\n",
        "\n",
        "\n",
        "class YOLO11TrainerCLI:\n",
        "    \"\"\"Command-line interface for YOLO11 training with improved dataset handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.all_classes = []\n",
        "        self.data_path = \"\"\n",
        "        self.dataset_manager = DatasetManager(print)\n",
        "        self.check_gpu()\n",
        "\n",
        "    def check_gpu(self):\n",
        "        \"\"\"Check for available GPUs with better error handling\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è PyTorch not available, cannot check GPU status\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_count = torch.cuda.device_count()\n",
        "                gpu_names = []\n",
        "                for i in range(gpu_count):\n",
        "                    try:\n",
        "                        gpu_names.append(torch.cuda.get_device_name(i))\n",
        "                    except Exception:\n",
        "                        gpu_names.append(f\"GPU {i}\")\n",
        "\n",
        "                print(f\"‚úÖ {gpu_count} GPU(s) available: {', '.join(gpu_names)}\")\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, range(gpu_count)))\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No GPU available, will use CPU\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è GPU check failed: {e}\")\n",
        "\n",
        "    def is_valid_zip(self, file_path):\n",
        "        \"\"\"Check if file is a valid ZIP archive\"\"\"\n",
        "        try:\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                bad_file = zip_ref.testzip()\n",
        "                if bad_file:\n",
        "                    print(f\"‚ö†Ô∏è ZIP file contains corrupted file: {bad_file}\")\n",
        "                    return False\n",
        "                return True\n",
        "        except zipfile.BadZipFile:\n",
        "            print(\"‚ùå File is not a valid ZIP file\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading ZIP file: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_data(self, zip_path):\n",
        "        \"\"\"Load and analyze the ZIP file with comprehensive error handling\"\"\"\n",
        "        self.data_path = zip_path\n",
        "\n",
        "        if not self.is_valid_zip(zip_path):\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            print(\"üöÄ Starting data loading process...\")\n",
        "\n",
        "            # Extract ZIP file\n",
        "            extract_path = \"./temp_data\"\n",
        "            if os.path.exists(extract_path):\n",
        "                shutil.rmtree(extract_path)\n",
        "            os.makedirs(extract_path)\n",
        "\n",
        "            print(\"üì¶ Extracting ZIP file...\")\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_path)\n",
        "\n",
        "            print(\"‚úÖ ZIP file extracted successfully\")\n",
        "\n",
        "            # Analyze dataset structure\n",
        "            structure_info = self.dataset_manager.analyze_dataset_structure(extract_path)\n",
        "\n",
        "            if structure_info['total_images'] == 0:\n",
        "                print(\"‚ùå No image files found in dataset\")\n",
        "                return False\n",
        "\n",
        "            if structure_info['total_labels'] == 0:\n",
        "                print(\"‚ùå No label files found in dataset\")\n",
        "                return False\n",
        "\n",
        "            # Reorganize dataset\n",
        "            if not self.dataset_manager.reorganize_dataset(extract_path, structure_info):\n",
        "                print(\"‚ùå Failed to reorganize dataset\")\n",
        "                return False\n",
        "\n",
        "            # Extract class information\n",
        "            all_class_ids = sorted(list(structure_info['class_ids']))\n",
        "            if not all_class_ids:\n",
        "                print(\"‚ùå No valid class labels found\")\n",
        "                return False\n",
        "\n",
        "            # Load class names\n",
        "            self.all_classes = self.load_class_names(extract_path, all_class_ids)\n",
        "\n",
        "            print(f\"‚úÖ Successfully loaded dataset with {len(self.all_classes)} classes:\")\n",
        "            for i, class_name in enumerate(self.all_classes):\n",
        "                print(f\"   {all_class_ids[i]:2d}: {class_name}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load data: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def load_class_names(self, extract_path, all_class_ids):\n",
        "        \"\"\"Load class names from various sources\"\"\"\n",
        "        class_names = [f\"Class_{i}\" for i in all_class_ids]\n",
        "\n",
        "        for root, dirs, files in os.walk(extract_path):\n",
        "            if 'classes.txt' in files:\n",
        "                try:\n",
        "                    with open(os.path.join(root, 'classes.txt'), 'r') as f:\n",
        "                        names = [line.strip() for line in f if line.strip()]\n",
        "                        if len(names) >= max(all_class_ids) + 1:\n",
        "                            class_names = [names[i] for i in all_class_ids]\n",
        "                            print(\"üìù Loaded class names from classes.txt\")\n",
        "                            return class_names\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Error reading classes.txt: {e}\")\n",
        "\n",
        "            for file in files:\n",
        "                if file.endswith(('.yaml', '.yml')):\n",
        "                    try:\n",
        "                        with open(os.path.join(root, file), 'r') as f:\n",
        "                            data = yaml.safe_load(f)\n",
        "                            if 'names' in data:\n",
        "                                names = data['names']\n",
        "                                if isinstance(names, list) and len(names) >= max(all_class_ids) + 1:\n",
        "                                    class_names = [names[i] for i in all_class_ids]\n",
        "                                    print(f\"üìù Loaded class names from {file}\")\n",
        "                                    return class_names\n",
        "                                elif isinstance(names, dict):\n",
        "                                    class_names = [names.get(i, f\"Class_{i}\") for i in all_class_ids]\n",
        "                                    print(f\"üìù Loaded class names from {file}\")\n",
        "                                    return class_names\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n",
        "\n",
        "        print(\"üìù Using default class names\")\n",
        "        return class_names\n",
        "\n",
        "    def select_classes_interactive(self):\n",
        "        \"\"\"Interactive class selection with better UX\"\"\"\n",
        "        print(f\"\\nüìã Available Classes ({len(self.all_classes)} total):\")\n",
        "\n",
        "        for i, class_name in enumerate(self.all_classes):\n",
        "            print(f\"   {i:2d}: {class_name}\")\n",
        "\n",
        "        print(f\"\\nüéØ Select classes to train:\")\n",
        "        print(\"   ‚Ä¢ Type 'all' for all classes\")\n",
        "        print(\"   ‚Ä¢ Type numbers separated by commas (e.g., 0,2,5)\")\n",
        "        print(\"   ‚Ä¢ Type ranges with dashes (e.g., 0-5,8,10-12)\")\n",
        "        print(\"   ‚Ä¢ Press Ctrl+C to cancel\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                selection = input(\"\\n‚û§ Enter your selection: \").strip()\n",
        "\n",
        "                if selection.lower() == 'all':\n",
        "                    selected_indices = list(range(len(self.all_classes)))\n",
        "                    break\n",
        "\n",
        "                selected_indices = self.parse_selection(selection)\n",
        "\n",
        "                if selected_indices:\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"‚ùå No valid classes selected. Please try again.\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nüö´ Selection cancelled by user.\")\n",
        "                sys.exit(0)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {e}. Please try again.\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Selected {len(selected_indices)} classes:\")\n",
        "        for i in selected_indices:\n",
        "            print(f\"   {i:2d}: {self.all_classes[i]}\")\n",
        "\n",
        "        return selected_indices\n",
        "\n",
        "    def parse_selection(self, selection):\n",
        "        \"\"\"Parse user selection string\"\"\"\n",
        "        selected_indices = []\n",
        "        parts = selection.split(',')\n",
        "\n",
        "        for part in parts:\n",
        "            part = part.strip()\n",
        "            if '-' in part:\n",
        "                try:\n",
        "                    start, end = map(int, part.split('-'))\n",
        "                    selected_indices.extend(range(start, end + 1))\n",
        "                except ValueError:\n",
        "                    print(f\"‚ùå Invalid range format: {part}\")\n",
        "                    continue\n",
        "            else:\n",
        "                try:\n",
        "                    selected_indices.append(int(part))\n",
        "                except ValueError:\n",
        "                    print(f\"‚ùå Invalid number: {part}\")\n",
        "                    continue\n",
        "\n",
        "        # Remove duplicates and validate\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        valid_indices = [i for i in selected_indices if 0 <= i < len(self.all_classes)]\n",
        "\n",
        "        if len(valid_indices) != len(selected_indices):\n",
        "            invalid = [i for i in selected_indices if i not in valid_indices]\n",
        "            print(f\"‚ö†Ô∏è Invalid indices ignored: {invalid}\")\n",
        "\n",
        "        return sorted(valid_indices)\n",
        "\n",
        "    def get_training_parameters(self):\n",
        "        \"\"\"Get training parameters from user\"\"\"\n",
        "        print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
        "        print(\"Press Enter to use default values shown in parentheses\")\n",
        "\n",
        "        params = {}\n",
        "\n",
        "        try:\n",
        "            epochs_input = input(\"Epochs (100): \").strip()\n",
        "            params['epochs'] = int(epochs_input) if epochs_input else 100\n",
        "\n",
        "            imgsz_input = input(\"Image size (640): \").strip()\n",
        "            params['imgsz'] = int(imgsz_input) if imgsz_input else 640\n",
        "\n",
        "            batch_input = input(\"Batch size (16): \").strip()\n",
        "            params['batch'] = int(batch_input) if batch_input else 16\n",
        "\n",
        "            models = [\"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\"]\n",
        "            print(f\"\\nAvailable models:\")\n",
        "            for i, model in enumerate(models):\n",
        "                print(f\"   {i}: {model}\")\n",
        "\n",
        "            model_input = input(\"Select model (0 for nano): \").strip()\n",
        "            model_idx = int(model_input) if model_input else 0\n",
        "            params['model'] = models[model_idx] if 0 <= model_idx < len(models) else models[0]\n",
        "\n",
        "        except ValueError:\n",
        "            print(\"‚ö†Ô∏è Invalid input detected. Using default parameters...\")\n",
        "            params = {'epochs': 100, 'imgsz': 640, 'batch': 16, 'model': 'yolo11n.pt'}\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüö´ Configuration cancelled by user.\")\n",
        "            sys.exit(0)\n",
        "\n",
        "        return params\n",
        "\n",
        "    def train_model(self, selected_indices, params):\n",
        "        \"\"\"Train YOLO model with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "            from ultralytics import YOLO\n",
        "        except ImportError as e:\n",
        "            print(f\"‚ùå Required packages not available: {e}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            print(f\"üöÄ Starting training with {len(selected_indices)} classes...\")\n",
        "\n",
        "            # Create dataset configuration\n",
        "            class_mapping = self.create_dataset_yaml(selected_indices)\n",
        "\n",
        "            # Filter labels\n",
        "            self.filter_labels(class_mapping)\n",
        "\n",
        "            # Final verification\n",
        "            if not self.final_dataset_check():\n",
        "                print(\"‚ùå Dataset verification failed\")\n",
        "                return False\n",
        "\n",
        "            # Initialize model\n",
        "            print(f\"ü§ñ Loading {params['model']} model...\")\n",
        "            model = YOLO(params['model'])\n",
        "\n",
        "            # Adjust batch size for GPU memory if needed\n",
        "            if torch.cuda.is_available():\n",
        "                try:\n",
        "                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                    if gpu_memory < 8 and params['batch'] > 8:\n",
        "                        params['batch'] = 8\n",
        "                        print(f\"üìâ Reduced batch size to {params['batch']} for GPU memory\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            print(\"üèÉ Training started...\")\n",
        "            print(f\"Parameters: epochs={params['epochs']}, imgsz={params['imgsz']}, batch={params['batch']}\")\n",
        "\n",
        "            yaml_path = os.path.abspath('./dataset.yaml')\n",
        "            print(f\"üìÑ Using dataset config: {yaml_path}\")\n",
        "\n",
        "            # Start training\n",
        "            results = model.train(\n",
        "                data=yaml_path,\n",
        "                epochs=params['epochs'],\n",
        "                imgsz=params['imgsz'],\n",
        "                batch=params['batch'],\n",
        "                device='0' if torch.cuda.is_available() else 'cpu',\n",
        "                project='./runs/train',\n",
        "                name='yolo11_custom',\n",
        "                exist_ok=True,\n",
        "                verbose=True,\n",
        "                patience=20,\n",
        "                save_period=max(10, params['epochs'] // 10)\n",
        "            )\n",
        "\n",
        "            print(\"üéâ Training completed successfully!\")\n",
        "            print(f\"üìÅ Model saved to: {results.save_dir}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed: {str(e)}\")\n",
        "            print(\"\\nüîß Troubleshooting tips:\")\n",
        "            print(\"‚Ä¢ Check dataset paths and file permissions\")\n",
        "            print(\"‚Ä¢ Try reducing batch size or image size\")\n",
        "            print(\"‚Ä¢ Verify all images and labels are valid\")\n",
        "            print(\"‚Ä¢ Check available disk space\")\n",
        "            return False\n",
        "\n",
        "    def create_dataset_yaml(self, selected_indices):\n",
        "        \"\"\"Create dataset.yaml configuration\"\"\"\n",
        "        class_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(selected_indices)}\n",
        "        selected_names = [self.all_classes[i] for i in selected_indices]\n",
        "\n",
        "        dataset_config = {\n",
        "            'path': os.path.abspath('./temp_data'),\n",
        "            'train': 'train/images',\n",
        "            'val': 'val/images',\n",
        "            'test': 'test/images',\n",
        "            'nc': len(selected_indices),\n",
        "            'names': selected_names\n",
        "        }\n",
        "\n",
        "        with open('./dataset.yaml', 'w') as f:\n",
        "            yaml.dump(dataset_config, f, default_flow_style=False)\n",
        "\n",
        "        print(f\"üìÑ Created dataset.yaml with {len(selected_indices)} classes\")\n",
        "        return class_mapping\n",
        "\n",
        "    def filter_labels(self, class_mapping):\n",
        "        \"\"\"Filter label files for selected classes\"\"\"\n",
        "        print(\"üîÑ Filtering labels...\")\n",
        "\n",
        "        for root, dirs, files in os.walk('./temp_data'):\n",
        "            if 'labels' in root:\n",
        "                for file in files:\n",
        "                    if file.endswith('.txt'):\n",
        "                        self.filter_label_file(os.path.join(root, file), class_mapping)\n",
        "\n",
        "    def filter_label_file(self, label_path, class_mapping):\n",
        "        \"\"\"Filter individual label file\"\"\"\n",
        "        try:\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            filtered_lines = []\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 5:\n",
        "                        try:\n",
        "                            class_id = int(parts[0])\n",
        "                            if class_id in class_mapping:\n",
        "                                parts[0] = str(class_mapping[class_id])\n",
        "                                filtered_lines.append(' '.join(parts) + '\\n')\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "            with open(label_path, 'w') as f:\n",
        "                f.writelines(filtered_lines)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error filtering {label_path}: {e}\")\n",
        "\n",
        "    def final_dataset_check(self):\n",
        "        \"\"\"Final comprehensive check before training\"\"\"\n",
        "        print(\"üîç Final dataset verification...\")\n",
        "\n",
        "        # Check required paths\n",
        "        required_paths = [\n",
        "            './temp_data/train/images',\n",
        "            './temp_data/train/labels',\n",
        "            './temp_data/val/images',\n",
        "            './temp_data/val/labels',\n",
        "            './dataset.yaml'\n",
        "        ]\n",
        "\n",
        "        for path in required_paths:\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"‚ùå Missing: {path}\")\n",
        "                return False\n",
        "\n",
        "        # Count files and verify content\n",
        "        train_images = len([f for f in os.listdir('./temp_data/train/images')\n",
        "                           if f.lower().endswith(tuple(self.dataset_manager.image_extensions))])\n",
        "        train_labels = len([f for f in os.listdir('./temp_data/train/labels') if f.endswith('.txt')])\n",
        "        val_images = len([f for f in os.listdir('./temp_data/val/images')\n",
        "                         if f.lower().endswith(tuple(self.dataset_manager.image_extensions))])\n",
        "        val_labels = len([f for f in os.listdir('./temp_data/val/labels') if f.endswith('.txt')])\n",
        "\n",
        "        print(f\"üìä Dataset summary:\")\n",
        "        print(f\"   üèãÔ∏è Training: {train_images} images, {train_labels} labels\")\n",
        "        print(f\"   ‚úÖ Validation: {val_images} images, {val_labels} labels\")\n",
        "\n",
        "        if train_images == 0:\n",
        "            print(\"‚ùå No training images found\")\n",
        "            return False\n",
        "\n",
        "        if val_images == 0:\n",
        "            print(\"‚ùå No validation images found\")\n",
        "            return False\n",
        "\n",
        "        # Verify dataset.yaml content\n",
        "        try:\n",
        "            with open('./dataset.yaml', 'r') as f:\n",
        "                config = yaml.safe_load(f)\n",
        "                required_keys = ['path', 'train', 'val', 'nc', 'names']\n",
        "                for key in required_keys:\n",
        "                    if key not in config:\n",
        "                        print(f\"‚ùå Missing key in dataset.yaml: {key}\")\n",
        "                        return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading dataset.yaml: {e}\")\n",
        "            return False\n",
        "\n",
        "        print(\"‚úÖ Final verification passed - ready for training!\")\n",
        "        return True\n",
        "\n",
        "    def run_interactive(self):\n",
        "        \"\"\"Run interactive CLI training\"\"\"\n",
        "        print(\"\\nüöÄ YOLO11 Training Tool (Interactive Mode)\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Get ZIP file\n",
        "        while True:\n",
        "            print(f\"\\nüìÅ Training Data:\")\n",
        "            zip_path = input(\"Enter path to ZIP file: \").strip().strip('\"\\'')\n",
        "\n",
        "            if not os.path.exists(zip_path):\n",
        "                print(\"‚ùå File not found. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            if not zip_path.lower().endswith('.zip'):\n",
        "                print(\"‚ùå Please provide a ZIP file.\")\n",
        "                continue\n",
        "\n",
        "            break\n",
        "\n",
        "        # Load data\n",
        "        if not self.load_data(zip_path):\n",
        "            print(\"‚ùå Failed to load data. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Select classes\n",
        "        selected_indices = self.select_classes_interactive()\n",
        "\n",
        "        # Get training parameters\n",
        "        params = self.get_training_parameters()\n",
        "\n",
        "        # Show summary\n",
        "        print(f\"\\nüìã Training Summary:\")\n",
        "        print(f\"   üìä Dataset: {os.path.basename(zip_path)}\")\n",
        "        print(f\"   üéØ Classes: {len(selected_indices)} selected\")\n",
        "        print(f\"   ü§ñ Model: {params['model']}\")\n",
        "        print(f\"   üìà Epochs: {params['epochs']}\")\n",
        "        print(f\"   üñºÔ∏è Image size: {params['imgsz']}\")\n",
        "        print(f\"   üì¶ Batch size: {params['batch']}\")\n",
        "\n",
        "        # Confirm training\n",
        "        print(f\"\\n‚ö° Ready to start training!\")\n",
        "        try:\n",
        "            confirm = input(\"Continue? (y/N): \").strip().lower()\n",
        "            if confirm in ['y', 'yes']:\n",
        "                success = self.train_model(selected_indices, params)\n",
        "                if success:\n",
        "                    print(\"\\nüéâ Training completed successfully!\")\n",
        "                    print(\"üìÅ Check './runs/train/yolo11_custom' for results\")\n",
        "                else:\n",
        "                    print(\"\\n‚ùå Training failed.\")\n",
        "            else:\n",
        "                print(\"üö´ Training cancelled.\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüö´ Training cancelled by user.\")\n",
        "\n",
        "\n",
        "def train_yolo_simple(zip_path=None, classes=\"all\", epochs=100, imgsz=640, batch=16, model=\"yolo11n.pt\"):\n",
        "    \"\"\"\n",
        "    Simple function for Jupyter/Colab environments with robust dataset handling\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üöÄ YOLO11 Simple Training\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # Check packages first\n",
        "    if not check_and_install_packages():\n",
        "        print(\"‚ùå Failed to install required packages\")\n",
        "        return\n",
        "\n",
        "    cli = YOLO11TrainerCLI()\n",
        "\n",
        "    if zip_path is None:\n",
        "        print(\"Interactive mode - please provide input when prompted\")\n",
        "        cli.run_interactive()\n",
        "        return\n",
        "\n",
        "    # Validate ZIP file\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"‚ùå File not found: {zip_path}\")\n",
        "        return\n",
        "\n",
        "    if not zip_path.lower().endswith('.zip'):\n",
        "        print(f\"‚ùå File is not a ZIP file: {zip_path}\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    if not cli.load_data(zip_path):\n",
        "        print(\"‚ùå Failed to load data\")\n",
        "        return\n",
        "\n",
        "    # Parse classes\n",
        "    if isinstance(classes, str):\n",
        "        if classes.lower() == \"all\":\n",
        "            selected_indices = list(range(len(cli.all_classes)))\n",
        "        else:\n",
        "            try:\n",
        "                selected_indices = [int(x.strip()) for x in classes.split(',')]\n",
        "            except ValueError:\n",
        "                print(\"‚ùå Invalid class format. Use 'all' or '0,1,2'\")\n",
        "                return\n",
        "    elif isinstance(classes, list):\n",
        "        selected_indices = classes\n",
        "    else:\n",
        "        print(\"‚ùå Classes must be 'all', '0,1,2', or [0,1,2]\")\n",
        "        return\n",
        "\n",
        "    # Validate indices\n",
        "    valid_indices = [i for i in selected_indices if 0 <= i < len(cli.all_classes)]\n",
        "    if len(valid_indices) != len(selected_indices):\n",
        "        print(\"‚ö†Ô∏è Some class indices were invalid and ignored\")\n",
        "\n",
        "    if not valid_indices:\n",
        "        print(\"‚ùå No valid class indices provided\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ Selected classes: {[cli.all_classes[i] for i in valid_indices]}\")\n",
        "\n",
        "    # Train model\n",
        "    params = {\n",
        "        'epochs': epochs,\n",
        "        'imgsz': imgsz,\n",
        "        'batch': batch,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "    success = cli.train_model(valid_indices, params)\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nüéâ Training completed successfully!\")\n",
        "        print(\"üìÅ Check './runs/train/yolo11_custom' for results\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Training failed. Check error messages above.\")\n",
        "\n",
        "\n",
        "def is_jupyter_environment():\n",
        "    \"\"\"Check if running in Jupyter/Colab environment\"\"\"\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        return get_ipython() is not None\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main entry point with improved error handling\"\"\"\n",
        "    print(\"üöÄ YOLO11 Training Tool - Initializing...\")\n",
        "\n",
        "    # Check and install packages first\n",
        "    if not check_and_install_packages():\n",
        "        print(\"‚ùå Failed to set up required packages. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Check environment\n",
        "    if is_jupyter_environment():\n",
        "        print(\"üî¨ Detected Jupyter/Colab environment\")\n",
        "        print(\"üí° Use train_yolo_simple() function for easy training:\")\n",
        "        print(\"   train_yolo_simple('/path/to/data.zip', classes='all', epochs=100)\")\n",
        "        print(\"\\nüìã Starting interactive CLI mode...\")\n",
        "        cli = YOLO11TrainerCLI()\n",
        "        cli.run_interactive()\n",
        "        return\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='YOLO11 Training Tool - Fixed dataset handling version',\n",
        "        epilog='''\n",
        "Examples:\n",
        "  python yolo11_trainer.py --cli\n",
        "  python yolo11_trainer.py --zip data.zip --classes all --epochs 50\n",
        "  python yolo11_trainer.py --zip data.zip --classes 0,2,5 --epochs 100\n",
        "        ''',\n",
        "        formatter_class=argparse.RawDescriptionHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument('--gui', action='store_true', help='Force GUI mode')\n",
        "    parser.add_argument('--cli', action='store_true', help='Force CLI mode')\n",
        "    parser.add_argument('--zip', type=str, help='Path to training data ZIP file')\n",
        "    parser.add_argument('--classes', type=str, help='Class indices (\"all\" or \"0,1,2\")')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')\n",
        "    parser.add_argument('--imgsz', type=int, default=640, help='Image size')\n",
        "    parser.add_argument('--batch', type=int, default=16, help='Batch size')\n",
        "    parser.add_argument('--model', type=str, default='yolo11n.pt', help='YOLO model')\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args()\n",
        "    except SystemExit:\n",
        "        print(\"Starting interactive CLI mode...\")\n",
        "        cli = YOLO11TrainerCLI()\n",
        "        cli.run_interactive()\n",
        "        return\n",
        "\n",
        "    # Determine interface mode\n",
        "    if args.cli or (not GUI_AVAILABLE and not args.gui):\n",
        "        # CLI mode\n",
        "        cli = YOLO11TrainerCLI()\n",
        "\n",
        "        if args.zip and args.classes:\n",
        "            # Non-interactive mode\n",
        "            if cli.load_data(args.zip):\n",
        "                if args.classes.lower() == 'all':\n",
        "                    selected_indices = list(range(len(cli.all_classes)))\n",
        "                else:\n",
        "                    selected_indices = cli.parse_selection(args.classes)\n",
        "\n",
        "                if selected_indices:\n",
        "                    params = {\n",
        "                        'epochs': args.epochs,\n",
        "                        'imgsz': args.imgsz,\n",
        "                        'batch': args.batch,\n",
        "                        'model': args.model\n",
        "                    }\n",
        "                    cli.train_model(selected_indices, params)\n",
        "                else:\n",
        "                    print(\"‚ùå No valid class indices provided\")\n",
        "        else:\n",
        "            # Interactive mode\n",
        "            cli.run_interactive()\n",
        "\n",
        "    elif args.gui or GUI_AVAILABLE:\n",
        "        # GUI mode\n",
        "        if not GUI_AVAILABLE:\n",
        "            print(\"‚ùå GUI not available. Use --cli flag.\")\n",
        "            return\n",
        "\n",
        "        root = tk.Tk()\n",
        "        app = YOLO11Trainer(root)\n",
        "        root.mainloop()\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå No interface available. Use --cli flag.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "else:\n",
        "    # When imported as module\n",
        "    if is_jupyter_environment():\n",
        "        print(\"üî¨ YOLO11 Training Tool loaded in Jupyter/Colab\")\n",
        "        print(\"üí° Quick start:\")\n",
        "        print(\"   train_yolo_simple('/path/to/data.zip', classes='all', epochs=100)\")\n",
        "        print(\"   train_yolo_simple('/path/to/data.zip', classes='0,2,5', epochs=50)\")"
      ],
      "metadata": {
        "id": "pTSjXSemXuYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained Model Improvement with additional epoches"
      ],
      "metadata": {
        "id": "AhurRhpMrkhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOLO11 Model Improvement - Strategy 1: Optimized Fine-tuning (FIXED)\n",
        "===================================================================\n",
        "\n",
        "This script continues training your existing YOLO11 model with optimized settings\n",
        "to improve performance, especially for Traffic Light and Central Line detection.\n",
        "\n",
        "Current Performance:\n",
        "- Overall mAP50: 77.2%\n",
        "- Traffic Light: 57.6% mAP50 (CRITICAL - needs major improvement)\n",
        "- Central Line: 75.4% mAP50 (needs improvement)\n",
        "\n",
        "Target Performance:\n",
        "- Overall mAP50: 83%+\n",
        "- Traffic Light: 76%+ mAP50\n",
        "- Central Line: 83%+ mAP50\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def print_current_status():\n",
        "    \"\"\"Print current model status and improvement goals\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ YOLO11 Traffic Detection Model Improvement\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üìä Current Performance:\")\n",
        "    print(\"   Overall mAP50: 77.2%\")\n",
        "    print(\"   Overall Recall: 65.1%\")\n",
        "    print(\"   Traffic Light: 57.6% mAP50 (üö® CRITICAL)\")\n",
        "    print(\"   Central Line: 75.4% mAP50 (‚ö†Ô∏è POOR)\")\n",
        "    print(\"   Lane: 85.8% mAP50 (‚úÖ GOOD)\")\n",
        "    print(\"\")\n",
        "    print(\"üéØ Target Goals:\")\n",
        "    print(\"   Overall mAP50: 83%+\")\n",
        "    print(\"   Overall Recall: 75%+\")\n",
        "    print(\"   Traffic Light: 76%+ mAP50\")\n",
        "    print(\"   Central Line: 83%+ mAP50\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "def verify_model_exists():\n",
        "    \"\"\"Verify that the trained model exists\"\"\"\n",
        "    model_path = './runs/train/yolo11_custom/weights/best.pt'\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"‚ùå ERROR: Trained model not found!\")\n",
        "        print(f\"   Expected location: {model_path}\")\n",
        "        print(\"\")\n",
        "        print(\"üí° Alternative locations to check:\")\n",
        "        print(\"   ‚Ä¢ ./runs/train/yolo11_custom/weights/last.pt\")\n",
        "        print(\"   ‚Ä¢ ./runs/train/*/weights/best.pt\")\n",
        "        print(\"   ‚Ä¢ ./best.pt\")\n",
        "        return False\n",
        "\n",
        "    print(f\"‚úÖ Found trained model: {model_path}\")\n",
        "    return True\n",
        "\n",
        "def train_strategy_1_optimized():\n",
        "    \"\"\"\n",
        "    Strategy 1: Optimized Fine-tuning for Traffic Detection\n",
        "\n",
        "    This strategy focuses on:\n",
        "    1. Lower learning rate for fine-tuning\n",
        "    2. Enhanced augmentation for small objects (traffic lights)\n",
        "    3. Optimized loss weights for better localization\n",
        "    4. Extended training with patience\n",
        "    \"\"\"\n",
        "\n",
        "    print_current_status()\n",
        "\n",
        "    # Verify model exists\n",
        "    if not verify_model_exists():\n",
        "        return None\n",
        "\n",
        "    print(\"\\nüîÑ Loading your trained model...\")\n",
        "\n",
        "    try:\n",
        "        # Load your existing trained model\n",
        "        model = YOLO('./runs/train/yolo11_custom/weights/best.pt')\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        print(\"üí° Try using 'last.pt' instead:\")\n",
        "        try:\n",
        "            model = YOLO('./runs/train/yolo11_custom/weights/last.pt')\n",
        "            print(\"‚úÖ Model loaded from last.pt!\")\n",
        "        except:\n",
        "            print(\"‚ùå Could not load model. Please check the path.\")\n",
        "            return None\n",
        "\n",
        "    print(\"\\nüöÄ Starting Strategy 1: Optimized Fine-tuning...\")\n",
        "    print(\"‚è±Ô∏è  Estimated training time: ~45-60 minutes\")\n",
        "    print(\"üìä Training for 200 additional epochs\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        results = model.train(\n",
        "            # üìÅ CORE CONFIGURATION\n",
        "            data='./dataset.yaml',              # Your existing dataset\n",
        "            epochs=200,                         # Extended training (200 MORE epochs)\n",
        "            imgsz=640,                          # Image size\n",
        "            batch=16,                           # Batch size\n",
        "            device='0',                         # GPU device\n",
        "            project='./runs/train',             # Project directory\n",
        "            name='yolo11_traffic_optimized',    # New experiment name\n",
        "            exist_ok=True,                      # Overwrite if exists\n",
        "\n",
        "            # üéØ LEARNING RATE OPTIMIZATION (Fine-tuning settings)\n",
        "            lr0=0.0003,                         # Lower initial learning rate for fine-tuning\n",
        "            lrf=0.005,                          # Very low final learning rate (0.5% of initial)\n",
        "            momentum=0.9,                       # Momentum for SGD optimizer\n",
        "            weight_decay=0.0005,                # Weight decay for regularization\n",
        "            warmup_epochs=5,                    # Warmup epochs for stable start\n",
        "            warmup_momentum=0.8,                # Warmup momentum\n",
        "            warmup_bias_lr=0.1,                 # Warmup bias learning rate\n",
        "\n",
        "            # ‚è∞ TRAINING CONTROL\n",
        "            patience=50,                        # Early stopping patience (more than default)\n",
        "            close_mosaic=10,                    # Epochs to close mosaic augmentation\n",
        "\n",
        "            # üîß LOSS FUNCTION WEIGHTS (Optimized for traffic detection)\n",
        "            box=10.0,                           # Box loss weight (INCREASED for better localization)\n",
        "            cls=1.0,                            # Classification loss weight (balanced)\n",
        "            dfl=2.0,                            # Distribution focal loss weight (better edges)\n",
        "\n",
        "            # üìà DATA AUGMENTATION (Optimized for road scenes and small objects)\n",
        "            hsv_h=0.015,                        # Hue augmentation (slight color variation)\n",
        "            hsv_s=0.8,                          # Saturation augmentation (strong - traffic lights vary)\n",
        "            hsv_v=0.6,                          # Value/brightness augmentation (day/night variation)\n",
        "            degrees=10,                         # Rotation augmentation (roads don't rotate much)\n",
        "            translate=0.15,                     # Translation augmentation (camera movement)\n",
        "            scale=0.8,                          # Scale augmentation (distance changes)\n",
        "            shear=1.0,                          # Shear augmentation (minimal for road scenes)\n",
        "            perspective=0.0,                    # Perspective augmentation (disabled)\n",
        "            flipud=0.0,                         # Vertical flip (disabled - roads don't flip)\n",
        "            fliplr=0.3,                         # Horizontal flip (some roads are bidirectional)\n",
        "\n",
        "            # üéØ ADVANCED AUGMENTATION (Critical for small object detection)\n",
        "            mosaic=1.0,                         # Mosaic augmentation (always enabled)\n",
        "            mixup=0.2,                          # Mixup augmentation (increased for variety)\n",
        "            copy_paste=0.4,                     # Copy-paste augmentation (helps small objects)\n",
        "\n",
        "            # üîç DETECTION SETTINGS\n",
        "            conf=0.001,                         # Confidence threshold for NMS\n",
        "            iou=0.6,                            # IoU threshold for NMS\n",
        "            max_det=300,                        # Maximum detections per image\n",
        "\n",
        "            # üìä TRAINING SETTINGS\n",
        "            workers=8,                          # Number of worker threads\n",
        "            seed=0,                             # Random seed for reproducibility\n",
        "            deterministic=True,                 # Deterministic training\n",
        "            single_cls=False,                   # Multi-class training\n",
        "            rect=False,                         # Rectangular training (disabled for augmentation)\n",
        "            cos_lr=True,                        # Cosine learning rate scheduler\n",
        "\n",
        "            # üíæ SAVING AND MONITORING\n",
        "            save=True,                          # Save checkpoints\n",
        "            save_period=20,                     # Save every 20 epochs\n",
        "            cache=False,                        # Cache images to RAM (disabled to save memory)\n",
        "            plots=True,                         # Generate training plots\n",
        "            overlap_mask=True,                  # Overlap mask for segmentation\n",
        "            mask_ratio=4,                       # Mask downsample ratio\n",
        "            dropout=0.0,                        # Dropout (disabled)\n",
        "            val=True,                           # Validate during training\n",
        "            split='val',                        # Validation split\n",
        "            verbose=True,                       # Verbose output\n",
        "\n",
        "            # üéõÔ∏è OPTIMIZER SETTINGS\n",
        "            optimizer='SGD',                    # Optimizer type\n",
        "            amp=True,                           # Automatic Mixed Precision\n",
        "            fraction=1.0,                       # Dataset fraction to use\n",
        "            profile=False,                      # Profile ONNX and TensorRT speeds\n",
        "            freeze=None,                        # Freeze layers\n",
        "            multi_scale=False,                  # Multi-scale training\n",
        "\n",
        "            # üéØ ADDITIONAL SETTINGS\n",
        "            nbs=64,                             # Nominal batch size\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"‚è±Ô∏è  Training time: {training_time/3600:.1f} hours\")\n",
        "        print(f\"üìÅ Results saved to: {results.save_dir}\")\n",
        "        print(f\"üèÜ Best model: {results.save_dir}/weights/best.pt\")\n",
        "        print(f\"üìä Last model: {results.save_dir}/weights/last.pt\")\n",
        "\n",
        "        # Print improvement expectations\n",
        "        print(\"\\nüìà Expected Improvements:\")\n",
        "        print(\"   ‚Ä¢ Traffic Light mAP50: 57.6% ‚Üí 76%+ (target)\")\n",
        "        print(\"   ‚Ä¢ Central Line mAP50: 75.4% ‚Üí 83%+ (target)\")\n",
        "        print(\"   ‚Ä¢ Overall mAP50: 77.2% ‚Üí 83%+ (target)\")\n",
        "        print(\"   ‚Ä¢ Overall Recall: 65.1% ‚Üí 75%+ (target)\")\n",
        "\n",
        "        print(\"\\nüîç To validate your improved model:\")\n",
        "        print(\"   model = YOLO('./runs/train/yolo11_traffic_optimized/weights/best.pt')\")\n",
        "        print(\"   results = model.val(data='./dataset.yaml')\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Training failed with error: {e}\")\n",
        "        print(\"\\nüîß Troubleshooting tips:\")\n",
        "        print(\"   ‚Ä¢ Check GPU memory (reduce batch size if needed)\")\n",
        "        print(\"   ‚Ä¢ Verify dataset.yaml exists and is correct\")\n",
        "        print(\"   ‚Ä¢ Ensure sufficient disk space\")\n",
        "        print(\"   ‚Ä¢ Try reducing image size to 416 if memory issues\")\n",
        "        return None\n",
        "\n",
        "def validate_improved_model():\n",
        "    \"\"\"Validate the improved model and compare with original performance\"\"\"\n",
        "    print(\"\\nüîç Validating improved model...\")\n",
        "\n",
        "    improved_model_path = './runs/train/yolo11_traffic_optimized/weights/best.pt'\n",
        "\n",
        "    if not os.path.exists(improved_model_path):\n",
        "        print(\"‚ùå Improved model not found. Train the model first.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Load improved model\n",
        "        model = YOLO(improved_model_path)\n",
        "\n",
        "        # Validate on dataset\n",
        "        results = model.val(data='./dataset.yaml')\n",
        "\n",
        "        print(\"‚úÖ Validation completed!\")\n",
        "        print(\"üìä Check the results above to see improvement\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Validation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run Strategy 1 training\"\"\"\n",
        "    print(\"üöÄ YOLO11 Traffic Detection - Strategy 1: Optimized Fine-tuning\")\n",
        "    print(\"================================================================\")\n",
        "\n",
        "    # Check if GPU is available\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "            print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No GPU detected. Training will use CPU (much slower).\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  PyTorch not found. Please install PyTorch first.\")\n",
        "        return\n",
        "\n",
        "    # Start training\n",
        "    results = train_strategy_1_optimized()\n",
        "\n",
        "    if results:\n",
        "        # Validate the improved model\n",
        "        validate_improved_model()\n",
        "\n",
        "        print(\"\\n‚úÖ Training Strategy 1 completed successfully!\")\n",
        "        print(\"üéØ Your model should now have significantly better performance\")\n",
        "        print(\"   especially for Traffic Light and Central Line detection.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Training failed. Please check the error messages above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "darn0OZMraul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SegFormer Transfer Learning"
      ],
      "metadata": {
        "id": "q1XSTZ5AIAXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "import glob\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import json\n",
        "import random\n",
        "\n",
        "class AutoConfig:\n",
        "    \"\"\"Automatically detect and configure paths and settings\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str = \".\"):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.config = self._auto_detect_setup()\n",
        "\n",
        "    def _auto_detect_setup(self) -> Dict:\n",
        "        config = {}\n",
        "\n",
        "        # Auto-detect YOLO model\n",
        "        config['yolo_model'] = self._find_yolo_model()\n",
        "\n",
        "        # Auto-detect dataset configuration\n",
        "        config['dataset_config'] = self._find_dataset_config()\n",
        "\n",
        "        # Auto-detect data paths\n",
        "        config['data_paths'] = self._find_data_paths()\n",
        "\n",
        "        # Set up output directories\n",
        "        config['output_dirs'] = self._setup_output_dirs()\n",
        "\n",
        "        return config\n",
        "\n",
        "    def _find_yolo_model(self) -> str:\n",
        "        \"\"\"Find the best YOLO model to use\"\"\"\n",
        "        # Priority order: trained model > yolo11n.pt > default\n",
        "        possible_paths = [\n",
        "            \"runs/detect/train*/weights/best.pt\",\n",
        "            \"yolo11n.pt\",\n",
        "            \"yolo11s.pt\",\n",
        "            \"yolo11m.pt\"\n",
        "        ]\n",
        "\n",
        "        for pattern in possible_paths:\n",
        "            matches = list(self.base_dir.glob(pattern))\n",
        "            if matches:\n",
        "                # Get the most recent if multiple matches\n",
        "                latest = max(matches, key=lambda p: p.stat().st_mtime)\n",
        "                print(f\"Found YOLO model: {latest}\")\n",
        "                return str(latest)\n",
        "\n",
        "        # Fallback to downloading yolo11n.pt\n",
        "        print(\"No YOLO model found, will use yolo11n.pt (will download if needed)\")\n",
        "        return \"yolo11n.pt\"\n",
        "\n",
        "    def _find_dataset_config(self) -> Dict:\n",
        "        \"\"\"Find and parse dataset configuration\"\"\"\n",
        "        yaml_files = list(self.base_dir.glob(\"*.yaml\")) + list(self.base_dir.glob(\"dataset.yaml\"))\n",
        "\n",
        "        if yaml_files:\n",
        "            yaml_file = yaml_files[0]\n",
        "            print(f\"Found dataset config: {yaml_file}\")\n",
        "\n",
        "            try:\n",
        "                with open(yaml_file, 'r') as f:\n",
        "                    dataset_config = yaml.safe_load(f)\n",
        "\n",
        "                # Extract class information\n",
        "                if 'names' in dataset_config:\n",
        "                    classes = dataset_config['names']\n",
        "                    if isinstance(classes, dict):\n",
        "                        num_classes = len(classes)\n",
        "                        class_names = list(classes.values())\n",
        "                    else:\n",
        "                        num_classes = len(classes)\n",
        "                        class_names = classes\n",
        "                else:\n",
        "                    # Default classes based on your screenshot\n",
        "                    class_names = ['Background', 'Central line', 'Crosswalk', 'Lane',\n",
        "                                  'Separation', 'Traffic light', 'Traffic sign']\n",
        "                    num_classes = len(class_names)\n",
        "\n",
        "                return {\n",
        "                    'num_classes': num_classes,\n",
        "                    'class_names': class_names,\n",
        "                    'config_path': str(yaml_file)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading YAML: {e}\")\n",
        "\n",
        "        # Default configuration\n",
        "        return {\n",
        "            'num_classes': 7,\n",
        "            'class_names': ['Background', 'Central line', 'Crosswalk', 'Lane',\n",
        "                           'Separation', 'Traffic light', 'Traffic sign'],\n",
        "            'config_path': None\n",
        "        }\n",
        "\n",
        "    def _find_data_paths(self) -> Dict:\n",
        "        \"\"\"Auto-detect training and validation data paths\"\"\"\n",
        "        data_paths = {\n",
        "            'train_images': [],\n",
        "            'train_masks': [],\n",
        "            'val_images': [],\n",
        "            'val_masks': []\n",
        "        }\n",
        "\n",
        "        # Common data directory patterns\n",
        "        possible_dirs = [\n",
        "            'sample_data',\n",
        "            'temp_data',\n",
        "            'data',\n",
        "            'dataset',\n",
        "            'images',\n",
        "            '.'\n",
        "        ]\n",
        "\n",
        "        all_images = []\n",
        "        all_masks = []\n",
        "\n",
        "        for data_dir in possible_dirs:\n",
        "            data_path = self.base_dir / data_dir\n",
        "            if data_path.exists():\n",
        "                print(f\"Scanning directory: {data_path}\")\n",
        "\n",
        "                # Find all images recursively\n",
        "                img_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
        "                for ext in img_extensions:\n",
        "                    all_images.extend(glob.glob(str(data_path / \"**\" / ext), recursive=True))\n",
        "\n",
        "                # Find all masks recursively\n",
        "                mask_extensions = ['*.png', '*.jpg', '*.jpeg']\n",
        "                for ext in mask_extensions:\n",
        "                    potential_masks = glob.glob(str(data_path / \"**\" / ext), recursive=True)\n",
        "                    # Filter masks (usually in folders with 'mask', 'label', 'gt' in name)\n",
        "                    for mask_path in potential_masks:\n",
        "                        if any(keyword in mask_path.lower() for keyword in ['mask', 'label', 'gt', 'seg']):\n",
        "                            all_masks.append(mask_path)\n",
        "\n",
        "        # Remove duplicates and sort\n",
        "        all_images = sorted(list(set(all_images)))\n",
        "        all_masks = sorted(list(set(all_masks)))\n",
        "\n",
        "        print(f\"Found {len(all_images)} total images\")\n",
        "        print(f\"Found {len(all_masks)} total masks\")\n",
        "\n",
        "        # Try to match images with masks\n",
        "        matched_pairs = self._match_images_and_masks(all_images, all_masks)\n",
        "\n",
        "        if matched_pairs:\n",
        "            # Split matched pairs into train/val\n",
        "            random.shuffle(matched_pairs)\n",
        "            split_idx = int(0.8 * len(matched_pairs))\n",
        "\n",
        "            train_pairs = matched_pairs[:split_idx]\n",
        "            val_pairs = matched_pairs[split_idx:]\n",
        "\n",
        "            if train_pairs:\n",
        "                data_paths['train_images'], data_paths['train_masks'] = zip(*train_pairs)\n",
        "                data_paths['train_images'] = list(data_paths['train_images'])\n",
        "                data_paths['train_masks'] = list(data_paths['train_masks'])\n",
        "\n",
        "            if val_pairs:\n",
        "                data_paths['val_images'], data_paths['val_masks'] = zip(*val_pairs)\n",
        "                data_paths['val_images'] = list(data_paths['val_images'])\n",
        "                data_paths['val_masks'] = list(data_paths['val_masks'])\n",
        "\n",
        "            print(f\"Matched {len(matched_pairs)} image-mask pairs\")\n",
        "            print(f\"Train: {len(data_paths['train_images'])}, Val: {len(data_paths['val_images'])}\")\n",
        "\n",
        "        # Fallback: use all images without masks for inference\n",
        "        elif all_images:\n",
        "            print(\"No masks found, using images for inference only\")\n",
        "            split_idx = int(0.8 * len(all_images))\n",
        "            data_paths['train_images'] = all_images[:split_idx]\n",
        "            data_paths['val_images'] = all_images[split_idx:]\n",
        "            data_paths['train_masks'] = []\n",
        "            data_paths['val_masks'] = []\n",
        "\n",
        "        return data_paths\n",
        "\n",
        "    def _match_images_and_masks(self, images: List[str], masks: List[str]) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Match images with corresponding masks\"\"\"\n",
        "        matched_pairs = []\n",
        "\n",
        "        for img_path in images:\n",
        "            img_name = Path(img_path).stem\n",
        "            img_dir = str(Path(img_path).parent)\n",
        "\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            for mask_path in masks:\n",
        "                mask_name = Path(mask_path).stem\n",
        "                mask_dir = str(Path(mask_path).parent)\n",
        "\n",
        "                # Calculate similarity score\n",
        "                score = 0\n",
        "                if img_name in mask_name or mask_name in img_name:\n",
        "                    score += 3\n",
        "                if img_name == mask_name:\n",
        "                    score += 5\n",
        "                if 'mask' in mask_dir.lower() and 'image' in img_dir.lower():\n",
        "                    score += 2\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_match = mask_path\n",
        "\n",
        "            if best_match and best_score >= 2:\n",
        "                matched_pairs.append((img_path, best_match))\n",
        "\n",
        "        return matched_pairs\n",
        "\n",
        "    def _setup_output_dirs(self) -> Dict:\n",
        "        \"\"\"Create output directories\"\"\"\n",
        "        output_dirs = {\n",
        "            'segformer_results': self.base_dir / 'segformer_results',\n",
        "            'models': self.base_dir / 'segformer_results' / 'models',\n",
        "            'predictions': self.base_dir / 'segformer_results' / 'predictions'\n",
        "        }\n",
        "\n",
        "        for dir_path in output_dirs.values():\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        return {k: str(v) for k, v in output_dirs.items()}\n",
        "\n",
        "class AutoSegmentationDataset(Dataset):\n",
        "    \"\"\"Automatically configured segmentation dataset\"\"\"\n",
        "\n",
        "    def __init__(self, image_paths: List[str], mask_paths: List[str],\n",
        "                 processor: SegformerImageProcessor, num_classes: int,\n",
        "                 augmentations: Optional[A.Compose] = None, is_training: bool = True):\n",
        "\n",
        "        self.image_paths = sorted(image_paths)\n",
        "        self.mask_paths = sorted(mask_paths) if mask_paths else []\n",
        "        self.processor = processor\n",
        "        self.num_classes = num_classes\n",
        "        self.augmentations = augmentations if is_training else None\n",
        "        self.is_training = is_training\n",
        "        self.has_masks = len(self.mask_paths) > 0\n",
        "\n",
        "        print(f\"Dataset created: {len(self.image_paths)} images, {len(self.mask_paths)} masks\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        try:\n",
        "            image = cv2.imread(self.image_paths[idx])\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Could not load image: {self.image_paths[idx]}\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
        "            # Return a dummy black image\n",
        "            image = np.zeros((512, 512, 3), dtype=np.uint8)\n",
        "\n",
        "        # Load mask if available\n",
        "        if self.has_masks and idx < len(self.mask_paths):\n",
        "            try:\n",
        "                mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "                if mask is None:\n",
        "                    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "            except:\n",
        "                mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "        else:\n",
        "            # Create dummy mask for inference\n",
        "            mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "\n",
        "        # CRITICAL FIX: Ensure image and mask have same dimensions\n",
        "        if image.shape[:2] != mask.shape[:2]:\n",
        "            # Resize mask to match image dimensions\n",
        "            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "            print(f\"‚ö†Ô∏è  Resized mask from {mask.shape} to match image {image.shape[:2]}\")\n",
        "\n",
        "        # CRITICAL FIX: Resize to standard size BEFORE augmentations\n",
        "        target_size = (512, 512)\n",
        "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
        "        mask = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Ensure mask values are within valid range\n",
        "        mask = np.clip(mask, 0, self.num_classes - 1)\n",
        "\n",
        "        # Apply augmentations (now both image and mask are same size)\n",
        "        if self.augmentations:\n",
        "            try:\n",
        "                augmented = self.augmentations(image=image, mask=mask)\n",
        "                image = augmented['image']\n",
        "                mask = augmented['mask']\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Augmentation failed: {e}\")\n",
        "                # Fallback: apply basic normalization manually\n",
        "                image = image.astype(np.float32) / 255.0\n",
        "                mean = np.array([0.485, 0.456, 0.406])\n",
        "                std = np.array([0.229, 0.224, 0.225])\n",
        "                image = (image - mean) / std\n",
        "\n",
        "        # Convert to tensor format\n",
        "        if isinstance(image, np.ndarray):\n",
        "            # If augmentation failed, convert manually\n",
        "            if image.dtype != np.float32:\n",
        "                image = image.astype(np.float32) / 255.0\n",
        "                mean = np.array([0.485, 0.456, 0.406])\n",
        "                std = np.array([0.229, 0.224, 0.225])\n",
        "                image = (image - mean) / std\n",
        "            pixel_values = torch.tensor(image).permute(2, 0, 1)\n",
        "        else:\n",
        "            # Already converted by augmentations\n",
        "            pixel_values = image\n",
        "\n",
        "        # Ensure consistent tensor size\n",
        "        if pixel_values.shape[-2:] != torch.Size([512, 512]):\n",
        "            pixel_values = torch.nn.functional.interpolate(\n",
        "                pixel_values.unsqueeze(0),\n",
        "                size=(512, 512),\n",
        "                mode='bilinear',\n",
        "                align_corners=False\n",
        "            ).squeeze(0)\n",
        "\n",
        "        # Convert mask to tensor and ensure correct size\n",
        "        mask = torch.tensor(mask, dtype=torch.long)\n",
        "        if mask.shape != torch.Size([512, 512]):\n",
        "            mask = torch.nn.functional.interpolate(\n",
        "                mask.unsqueeze(0).unsqueeze(0).float(),\n",
        "                size=(512, 512),\n",
        "                mode='nearest'\n",
        "            ).squeeze().long()\n",
        "\n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': mask,\n",
        "            'image_path': self.image_paths[idx]\n",
        "        }\n",
        "\n",
        "class SafeSegformerModel(nn.Module):\n",
        "    \"\"\"Safe wrapper for SegformerForSemanticSegmentation with proper class handling\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Load base model without the classification head\n",
        "        print(f\"Loading SegFormer model for {num_classes} classes...\")\n",
        "\n",
        "        try:\n",
        "            # First try with ignore_mismatched_sizes\n",
        "            self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
        "                model_name,\n",
        "                num_labels=num_classes,\n",
        "                ignore_mismatched_sizes=True\n",
        "            )\n",
        "            print(\"‚úÖ Model loaded successfully with ignore_mismatched_sizes=True\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load with ignore_mismatched_sizes: {e}\")\n",
        "\n",
        "            try:\n",
        "                # Fallback: Load base model and replace classifier manually\n",
        "                print(\"üîÑ Trying manual classifier replacement...\")\n",
        "                self.segformer = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
        "\n",
        "                # Replace the classifier head\n",
        "                original_classifier = self.segformer.decode_head.classifier\n",
        "                in_channels = original_classifier.in_channels\n",
        "\n",
        "                self.segformer.decode_head.classifier = nn.Conv2d(\n",
        "                    in_channels, num_classes, kernel_size=1, stride=1, padding=0\n",
        "                )\n",
        "\n",
        "                # Update config\n",
        "                self.segformer.config.num_labels = num_classes\n",
        "                print(\"‚úÖ Model loaded with manual classifier replacement\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"‚ùå Manual replacement failed: {e2}\")\n",
        "                raise RuntimeError(f\"Could not load SegFormer model: {e2}\")\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        return self.segformer(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "class AutoSegformerPipeline:\n",
        "    \"\"\"Automated Segformer + YOLO pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, auto_mode: bool = True):\n",
        "        self.config = AutoConfig().config if auto_mode else {}\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize components\n",
        "        self.processor = None\n",
        "        self.segformer_model = None\n",
        "        self.yolo_model = None\n",
        "\n",
        "        self._setup_models()\n",
        "\n",
        "    def _setup_models(self):\n",
        "        \"\"\"Setup all models automatically\"\"\"\n",
        "        # Setup SegFormer\n",
        "        model_name = \"nvidia/segformer-b2-finetuned-ade-512-512\"\n",
        "        self.processor = SegformerImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "        num_classes = self.config['dataset_config']['num_classes']\n",
        "        self.segformer_model = SafeSegformerModel(model_name, num_classes)\n",
        "        self.segformer_model.to(self.device)\n",
        "\n",
        "        # Setup YOLO\n",
        "        yolo_path = self.config['yolo_model']\n",
        "        try:\n",
        "            self.yolo_model = YOLO(yolo_path)\n",
        "            print(f\"‚úÖ YOLO model loaded: {yolo_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  YOLO loading warning: {e}\")\n",
        "            self.yolo_model = YOLO(\"yolo11n.pt\")  # Fallback\n",
        "\n",
        "        print(f\"‚úÖ SegFormer initialized for {num_classes} classes\")\n",
        "        print(f\"Class names: {self.config['dataset_config']['class_names']}\")\n",
        "\n",
        "    def create_dataloaders(self, batch_size: int = 4):\n",
        "        \"\"\"Create train and validation dataloaders automatically\"\"\"\n",
        "        data_paths = self.config['data_paths']\n",
        "\n",
        "        # Check if we have training data\n",
        "        if not data_paths['train_images']:\n",
        "            print(\"‚ùå No training images found!\")\n",
        "            return None, None\n",
        "\n",
        "        # Augmentations - Fixed with shape checking disabled and proper transforms\n",
        "        train_augs = A.Compose([\n",
        "            # Don't resize here since we do it manually before augmentations\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "            A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
        "            A.Blur(blur_limit=3, p=0.2),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(),\n",
        "        ], is_check_shapes=False)  # CRITICAL: Disable shape checking\n",
        "\n",
        "        val_augs = A.Compose([\n",
        "            # Don't resize here since we do it manually before augmentations\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(),\n",
        "        ], is_check_shapes=False)  # CRITICAL: Disable shape checking\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = AutoSegmentationDataset(\n",
        "            data_paths['train_images'],\n",
        "            data_paths['train_masks'],\n",
        "            self.processor,\n",
        "            self.config['dataset_config']['num_classes'],\n",
        "            train_augs,\n",
        "            is_training=True\n",
        "        )\n",
        "\n",
        "        # Create validation dataset (even if no masks)\n",
        "        val_dataset = None\n",
        "        if data_paths['val_images']:\n",
        "            val_dataset = AutoSegmentationDataset(\n",
        "                data_paths['val_images'],\n",
        "                data_paths['val_masks'],\n",
        "                self.processor,\n",
        "                self.config['dataset_config']['num_classes'],\n",
        "                val_augs,\n",
        "                is_training=False\n",
        "            )\n",
        "\n",
        "        # Custom collate function to handle any remaining size mismatches\n",
        "        def safe_collate_fn(batch):\n",
        "            \"\"\"Custom collate function that ensures all tensors are the same size\"\"\"\n",
        "            pixel_values = []\n",
        "            labels = []\n",
        "            image_paths = []\n",
        "\n",
        "            for item in batch:\n",
        "                # Ensure all pixel_values are (3, 512, 512)\n",
        "                pv = item['pixel_values']\n",
        "                if pv.shape != torch.Size([3, 512, 512]):\n",
        "                    pv = torch.nn.functional.interpolate(\n",
        "                        pv.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False\n",
        "                    ).squeeze(0)\n",
        "                pixel_values.append(pv)\n",
        "\n",
        "                # Ensure all labels are (512, 512)\n",
        "                lbl = item['labels']\n",
        "                if lbl.shape != torch.Size([512, 512]):\n",
        "                    lbl = torch.nn.functional.interpolate(\n",
        "                        lbl.unsqueeze(0).unsqueeze(0).float(), size=(512, 512), mode='nearest'\n",
        "                    ).squeeze().long()\n",
        "                labels.append(lbl)\n",
        "\n",
        "                image_paths.append(item['image_path'])\n",
        "\n",
        "            return {\n",
        "                'pixel_values': torch.stack(pixel_values),\n",
        "                'labels': torch.stack(labels),\n",
        "                'image_path': image_paths\n",
        "            }\n",
        "\n",
        "        # Create dataloaders with custom collate function\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            collate_fn=safe_collate_fn\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            collate_fn=safe_collate_fn\n",
        "        ) if val_dataset else None\n",
        "\n",
        "        print(f\"‚úÖ Created dataloaders:\")\n",
        "        print(f\"   Training: {len(train_dataset)} samples\")\n",
        "        print(f\"   Validation: {len(val_dataset) if val_dataset else 0} samples\")\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def train(self, num_epochs: int = 20, learning_rate: float = 1e-4, batch_size: int = 4):\n",
        "        \"\"\"Auto-training pipeline\"\"\"\n",
        "        print(\"üöÄ Starting automated training...\")\n",
        "\n",
        "        train_loader, val_loader = self.create_dataloaders(batch_size)\n",
        "\n",
        "        if train_loader is None or len(train_loader) == 0:\n",
        "            print(\"‚ùå No training data found! Please check your data paths.\")\n",
        "            return\n",
        "\n",
        "        # Setup optimizer and scheduler\n",
        "        optimizer = optim.AdamW(self.segformer_model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "        scheduler = optim.lr_scheduler.PolynomialLR(optimizer, total_iters=num_epochs, power=0.9)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training\n",
        "            self.segformer_model.train()\n",
        "            train_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "                try:\n",
        "                    pixel_values = batch['pixel_values'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = self.segformer_model(pixel_values=pixel_values, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    train_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "\n",
        "                    if batch_idx % 5 == 0:\n",
        "                        print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Training batch error: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation\n",
        "            val_loss = train_loss / max(num_batches, 1)  # Fallback if no val data\n",
        "\n",
        "            if val_loader and len(val_loader) > 0:\n",
        "                self.segformer_model.eval()\n",
        "                val_loss = 0.0\n",
        "                val_batches = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader:\n",
        "                        try:\n",
        "                            pixel_values = batch['pixel_values'].to(self.device)\n",
        "                            labels = batch['labels'].to(self.device)\n",
        "\n",
        "                            outputs = self.segformer_model(pixel_values=pixel_values, labels=labels)\n",
        "                            val_loss += outputs.loss.item()\n",
        "                            val_batches += 1\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ö†Ô∏è  Validation batch error: {e}\")\n",
        "                            continue\n",
        "\n",
        "                if val_batches > 0:\n",
        "                    val_loss = val_loss / val_batches\n",
        "\n",
        "            avg_train_loss = train_loss / max(num_batches, 1)\n",
        "\n",
        "            print(f'üìä Epoch {epoch+1}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "            # Save best model\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                model_path = os.path.join(self.config['output_dirs']['models'], 'best_segformer.pth')\n",
        "                torch.save(self.segformer_model.state_dict(), model_path)\n",
        "                print(f'üíæ New best model saved to: {model_path}')\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        print(\"üéâ Training completed!\")\n",
        "\n",
        "    def predict_combined(self, image_path: str, save_results: bool = True):\n",
        "        \"\"\"Combined YOLO + Segformer prediction\"\"\"\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"‚ùå Image not found: {image_path}\")\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            # Load image\n",
        "            image = cv2.imread(image_path)\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # YOLO Detection\n",
        "            yolo_results = self.yolo_model(image_path)\n",
        "\n",
        "            # SegFormer Segmentation\n",
        "            inputs = self.processor(image_rgb, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            self.segformer_model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = self.segformer_model(**inputs)\n",
        "\n",
        "            # Process segmentation\n",
        "            logits = outputs.logits\n",
        "            upsampled_logits = nn.functional.interpolate(\n",
        "                logits,\n",
        "                size=image_rgb.shape[:2],\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            segmentation_mask = upsampled_logits.argmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "            if save_results:\n",
        "                self._save_prediction_results(image_path, yolo_results, segmentation_mask, image_rgb)\n",
        "\n",
        "            return yolo_results, segmentation_mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Prediction error: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def _save_prediction_results(self, image_path: str, yolo_results, seg_mask: np.ndarray, original_image: np.ndarray):\n",
        "        \"\"\"Save prediction results\"\"\"\n",
        "        try:\n",
        "            output_dir = self.config['output_dirs']['predictions']\n",
        "            base_name = Path(image_path).stem\n",
        "\n",
        "            # Save segmentation mask\n",
        "            mask_path = os.path.join(output_dir, f'{base_name}_segmentation.png')\n",
        "            cv2.imwrite(mask_path, seg_mask.astype(np.uint8) * 50)  # Scale for visibility\n",
        "\n",
        "            # Save YOLO results\n",
        "            if yolo_results:\n",
        "                yolo_img = yolo_results[0].plot()\n",
        "                yolo_path = os.path.join(output_dir, f'{base_name}_yolo.jpg')\n",
        "                cv2.imwrite(yolo_path, yolo_img)\n",
        "\n",
        "            print(f\"üíæ Results saved to: {output_dir}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error saving results: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Automated main function\"\"\"\n",
        "    print(\"üöÄ === Automated SegFormer + YOLO11 Pipeline ===\")\n",
        "\n",
        "    try:\n",
        "        # Initialize automated pipeline\n",
        "        pipeline = AutoSegformerPipeline(auto_mode=True)\n",
        "\n",
        "        # Print configuration\n",
        "        print(\"\\nüìã Configuration detected:\")\n",
        "        print(f\"   YOLO model: {pipeline.config['yolo_model']}\")\n",
        "        print(f\"   Number of classes: {pipeline.config['dataset_config']['num_classes']}\")\n",
        "        print(f\"   Classes: {pipeline.config['dataset_config']['class_names']}\")\n",
        "        print(f\"   Training images: {len(pipeline.config['data_paths']['train_images'])}\")\n",
        "        print(f\"   Validation images: {len(pipeline.config['data_paths']['val_images'])}\")\n",
        "\n",
        "        # Ask user what to do\n",
        "        print(\"\\nüéØ What would you like to do?\")\n",
        "        print(\"1. Train SegFormer\")\n",
        "        print(\"2. Run inference only\")\n",
        "        print(\"3. Both (recommended)\")\n",
        "\n",
        "        choice = input(\"Enter choice (1/2/3): \").strip()\n",
        "\n",
        "        if choice in ['1', '3']:\n",
        "            # Training\n",
        "            epochs = int(input(\"Enter number of epochs (default 20): \") or \"20\")\n",
        "            batch_size = int(input(\"Enter batch size (default 8 for T4): \") or \"8\")\n",
        "\n",
        "            pipeline.train(num_epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "        if choice in ['2', '3']:\n",
        "            # Inference\n",
        "            test_images = input(\"Enter path to test image (or press Enter to use sample): \").strip()\n",
        "\n",
        "            if not test_images:\n",
        "                # Use any available image from validation or train set\n",
        "                if pipeline.config['data_paths']['val_images']:\n",
        "                    test_images = pipeline.config['data_paths']['val_images'][0]\n",
        "                elif pipeline.config['data_paths']['train_images']:\n",
        "                    test_images = pipeline.config['data_paths']['train_images'][0]\n",
        "\n",
        "            if test_images and os.path.exists(test_images):\n",
        "                print(f\"\\nüîç Running inference on: {test_images}\")\n",
        "                yolo_results, seg_mask = pipeline.predict_combined(test_images)\n",
        "\n",
        "                if yolo_results:\n",
        "                    print(\"üéØ YOLO Detection Results:\")\n",
        "                    if len(yolo_results[0].boxes) > 0:\n",
        "                        for i, box in enumerate(yolo_results[0].boxes):\n",
        "                            class_id = int(box.cls[0])\n",
        "                            confidence = float(box.conf[0])\n",
        "                            print(f\"   Object {i+1}: Class {class_id}, Confidence: {confidence:.3f}\")\n",
        "                    else:\n",
        "                        print(\"   No objects detected\")\n",
        "\n",
        "                if seg_mask is not None:\n",
        "                    print(f\"\\nüé® Segmentation Results:\")\n",
        "                    print(f\"   Mask shape: {seg_mask.shape}\")\n",
        "                    print(f\"   Unique classes: {np.unique(seg_mask)}\")\n",
        "\n",
        "            else:\n",
        "                print(\"‚ùå No test images found!\")\n",
        "\n",
        "        print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Pipeline error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iflCYJzqczai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMPLETE SEGFORMER VIDEO SEGMENTATION"
      ],
      "metadata": {
        "id": "9NrSm95Ewc1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FIXED SEGFORMER VIDEO SEGMENTATION FOR TRAFFIC DATASET\n",
        "# Upload video ‚Üí Process with trained SegFormer ‚Üí Download results\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED CONFIGURATION FOR YOUR TRAFFIC DATASET\n",
        "# =============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # FIXED: Path to your trained SegFormer model\n",
        "    'model_path': './segformer_results/models/best_segformer.pth',  # Your saved model\n",
        "    'base_model': 'nvidia/segformer-b2-finetuned-ade-512-512',    # Base pretrained model\n",
        "\n",
        "    # FIXED: Your actual traffic class names\n",
        "    'class_names': ['Background', 'Central line', 'Crosswalk', 'Lane',\n",
        "                   'Separation', 'Traffic light', 'Traffic sign'],\n",
        "\n",
        "    # Processing settings\n",
        "    'max_size': 640,            # Processing resolution (640 for speed, 1024 for quality)\n",
        "    'overlay_alpha': 0.6,       # Overlay transparency (0.0-1.0)\n",
        "    'fps_output': 30,           # Output video FPS\n",
        "\n",
        "    # FIXED: Colors for traffic classes (RGB)\n",
        "    'colors': {\n",
        "        0: [0, 0, 0],           # Background - black\n",
        "        1: [255, 0, 0],         # Central line - red\n",
        "        2: [0, 255, 0],         # Crosswalk - green\n",
        "        3: [0, 0, 255],         # Lane - blue\n",
        "        4: [255, 255, 0],       # Separation - yellow\n",
        "        5: [255, 0, 255],       # Traffic light - magenta\n",
        "        6: [0, 255, 255],       # Traffic sign - cyan\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üéØ SEGFORMER VIDEO SEGMENTATION - TRAFFIC DATASET VERSION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED MODEL LOADER CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class SafeSegformerModel(nn.Module):\n",
        "    \"\"\"Safe wrapper for loading your trained SegFormer model with auto class detection\"\"\"\n",
        "\n",
        "    def __init__(self, base_model_name: str, num_classes: int, model_path: str = None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Auto-detect number of classes from saved model if available\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            try:\n",
        "                print(\"üîç Auto-detecting number of classes from saved model...\")\n",
        "                state_dict = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "                # Find classifier weight shape to determine actual number of classes\n",
        "                classifier_key = None\n",
        "                for key in state_dict.keys():\n",
        "                    if 'classifier.weight' in key:\n",
        "                        classifier_key = key\n",
        "                        break\n",
        "\n",
        "                if classifier_key:\n",
        "                    actual_num_classes = state_dict[classifier_key].shape[0]\n",
        "                    print(f\"üìä Detected {actual_num_classes} classes in saved model\")\n",
        "                    self.num_classes = actual_num_classes\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è  Could not detect classes from model, using provided number\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error detecting classes: {e}, using provided number\")\n",
        "\n",
        "        # Load base model with detected/provided number of classes\n",
        "        print(f\"üèóÔ∏è  Creating model with {self.num_classes} classes...\")\n",
        "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
        "            base_model_name,\n",
        "            num_labels=self.num_classes,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.segformer(pixel_values=pixel_values)\n",
        "        return outputs\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED VIDEO PROCESSOR CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class VideoSegFormer:\n",
        "    def __init__(self, model_path, base_model, class_names, colors):\n",
        "        self.model_path = model_path\n",
        "        self.base_model = base_model\n",
        "        self.original_class_names = class_names\n",
        "        self.original_colors = colors\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        print(f\"üß† Loading SegFormer model...\")\n",
        "        print(f\"   Device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            # FIXED: Load processor from base model\n",
        "            self.processor = SegformerImageProcessor.from_pretrained(base_model)\n",
        "\n",
        "            # FIXED: Auto-detect classes and load model\n",
        "            initial_num_classes = len(class_names)\n",
        "            self.model = SafeSegformerModel(base_model, initial_num_classes, model_path)\n",
        "            self.num_classes = self.model.num_classes\n",
        "\n",
        "            # Adjust class names and colors based on detected number\n",
        "            self.class_names, self.colors = self._adjust_classes_and_colors()\n",
        "\n",
        "            print(f\"   Final classes: {self.num_classes}\")\n",
        "            print(f\"   Class mapping: {self.class_names}\")\n",
        "\n",
        "            # FIXED: Load your trained weights if available\n",
        "            if model_path and os.path.exists(model_path):\n",
        "                print(f\"üì• Loading trained weights from: {model_path}\")\n",
        "                state_dict = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "                # Try to load with strict=False to handle any remaining mismatches\n",
        "                try:\n",
        "                    self.model.load_state_dict(state_dict, strict=True)\n",
        "                    print(\"‚úÖ Trained weights loaded successfully (strict)!\")\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"‚ö†Ô∏è  Strict loading failed: {e}\")\n",
        "                    print(\"üîÑ Trying flexible loading...\")\n",
        "                    self.model.load_state_dict(state_dict, strict=False)\n",
        "                    print(\"‚úÖ Trained weights loaded successfully (flexible)!\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Trained weights not found: {model_path}\")\n",
        "                print(\"üîÑ Using base pretrained model...\")\n",
        "\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(\"‚úÖ Model ready for inference!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model loading error: {e}\")\n",
        "            raise Exception(f\"Failed to load model: {e}\")\n",
        "\n",
        "    def _adjust_classes_and_colors(self):\n",
        "        \"\"\"Adjust class names and colors based on detected number of classes\"\"\"\n",
        "\n",
        "        if self.num_classes == len(self.original_class_names):\n",
        "            # Perfect match\n",
        "            return self.original_class_names, self.original_colors\n",
        "\n",
        "        elif self.num_classes == len(self.original_class_names) - 1:\n",
        "            # Likely missing background class\n",
        "            print(\"üîÑ Adjusting for model without background class...\")\n",
        "            adjusted_names = self.original_class_names[1:]  # Remove background\n",
        "            adjusted_colors = {i: self.original_colors[i+1] for i in range(self.num_classes)}\n",
        "            return adjusted_names, adjusted_colors\n",
        "\n",
        "        else:\n",
        "            # Custom adjustment\n",
        "            print(f\"üîÑ Custom class adjustment: {self.num_classes} detected vs {len(self.original_class_names)} expected\")\n",
        "\n",
        "            # Use as many class names as we have\n",
        "            if self.num_classes <= len(self.original_class_names):\n",
        "                adjusted_names = self.original_class_names[:self.num_classes]\n",
        "                adjusted_colors = {i: self.original_colors[i] for i in range(self.num_classes)}\n",
        "            else:\n",
        "                # More classes than names\n",
        "                adjusted_names = self.original_class_names + [f\"Class_{i}\" for i in range(len(self.original_class_names), self.num_classes)]\n",
        "                adjusted_colors = self.original_colors.copy()\n",
        "                for i in range(len(self.original_colors), self.num_classes):\n",
        "                    adjusted_colors[i] = [np.random.randint(0, 255) for _ in range(3)]\n",
        "\n",
        "            return adjusted_names, adjusted_colors\n",
        "\n",
        "    def segment_frame(self, frame):\n",
        "        \"\"\"Segment a single frame\"\"\"\n",
        "        try:\n",
        "            # Resize if needed\n",
        "            original_h, original_w = frame.shape[:2]\n",
        "            if max(original_h, original_w) > CONFIG['max_size']:\n",
        "                scale = CONFIG['max_size'] / max(original_h, original_w)\n",
        "                new_w, new_h = int(original_w * scale), int(original_h * scale)\n",
        "                resized = cv2.resize(frame, (new_w, new_h))\n",
        "            else:\n",
        "                resized = frame\n",
        "                scale = 1.0\n",
        "\n",
        "            # Convert to RGB and process\n",
        "            rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # FIXED: Process with your processor\n",
        "            inputs = self.processor(rgb, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                logits = outputs.logits\n",
        "\n",
        "                # FIXED: Proper upsampling and prediction\n",
        "                upsampled_logits = nn.functional.interpolate(\n",
        "                    logits,\n",
        "                    size=rgb.shape[:2],\n",
        "                    mode=\"bilinear\",\n",
        "                    align_corners=False,\n",
        "                )\n",
        "                prediction = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
        "\n",
        "            # Resize back if needed\n",
        "            if scale != 1.0:\n",
        "                pred_pil = Image.fromarray(prediction.astype(np.uint8))\n",
        "                pred_pil = pred_pil.resize((original_w, original_h), Image.NEAREST)\n",
        "                prediction = np.array(pred_pil)\n",
        "\n",
        "            return prediction\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Frame processing error: {e}\")\n",
        "            return np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    def create_overlay(self, frame, prediction):\n",
        "        \"\"\"Create colored overlay for traffic classes\"\"\"\n",
        "        colored_mask = np.zeros_like(frame)\n",
        "\n",
        "        for class_id, color in self.colors.items():\n",
        "            if class_id > 0:  # Skip background\n",
        "                mask = prediction == class_id\n",
        "                colored_mask[mask] = color\n",
        "\n",
        "        # Blend\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        overlay = cv2.addWeighted(frame_rgb, 1-CONFIG['overlay_alpha'],\n",
        "                                 colored_mask, CONFIG['overlay_alpha'], 0)\n",
        "\n",
        "        return cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    def add_legend(self, frame, prediction):\n",
        "        \"\"\"Add legend showing detected traffic classes\"\"\"\n",
        "        unique_classes = np.unique(prediction)\n",
        "\n",
        "        # Handle different class indexing based on whether background is included\n",
        "        if self.num_classes == len(self.original_class_names) - 1:\n",
        "            # Model trained without background (classes 0-5 map to original 1-6)\n",
        "            detected = [c for c in unique_classes if 0 <= c < len(self.class_names)]\n",
        "        else:\n",
        "            # Model includes background or has different structure\n",
        "            detected = [c for c in unique_classes if c > 0 and c < len(self.class_names)]\n",
        "\n",
        "        if not detected:\n",
        "            return frame\n",
        "\n",
        "        # Legend parameters\n",
        "        legend_width = 250\n",
        "        legend_height = 30 + len(detected) * 25\n",
        "        x = frame.shape[1] - legend_width - 10\n",
        "        y = 10\n",
        "\n",
        "        # Background\n",
        "        cv2.rectangle(frame, (x-5, y-5), (x+legend_width+5, y+legend_height+5),\n",
        "                     (0, 0, 0), -1)\n",
        "        cv2.rectangle(frame, (x-5, y-5), (x+legend_width+5, y+legend_height+5),\n",
        "                     (255, 255, 255), 2)\n",
        "\n",
        "        # Title\n",
        "        cv2.putText(frame, \"Traffic Elements:\", (x, y+20), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                   0.6, (255, 255, 255), 2)\n",
        "\n",
        "        # Classes\n",
        "        for i, class_id in enumerate(detected):\n",
        "            y_pos = y + 40 + i * 25\n",
        "            color = self.colors.get(class_id, [255, 255, 255])\n",
        "\n",
        "            # Get class name based on indexing\n",
        "            if class_id < len(self.class_names):\n",
        "                class_name = self.class_names[class_id]\n",
        "            else:\n",
        "                class_name = f\"Class{class_id}\"\n",
        "\n",
        "            # Color box\n",
        "            cv2.rectangle(frame, (x, y_pos-10), (x+15, y_pos), color, -1)\n",
        "\n",
        "            # Text\n",
        "            cv2.putText(frame, class_name, (x+20, y_pos), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                       0.5, (255, 255, 255), 1)\n",
        "\n",
        "        return frame\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED PROCESSING FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def find_model_automatically():\n",
        "    \"\"\"Automatically find your trained model\"\"\"\n",
        "    possible_paths = [\n",
        "        './segformer_results/models/best_segformer.pth',\n",
        "        './segformer_results/best_segformer.pth',\n",
        "        './best_segformer.pth',\n",
        "        './models/best_segformer.pth'\n",
        "    ]\n",
        "\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"‚úÖ Found model: {path}\")\n",
        "            return path\n",
        "\n",
        "    print(\"‚ö†Ô∏è  No trained model found. Will use base pretrained model.\")\n",
        "    return None\n",
        "\n",
        "def process_video(input_file):\n",
        "    \"\"\"Process uploaded video with SegFormer\"\"\"\n",
        "\n",
        "    # FIXED: Auto-find model\n",
        "    model_path = find_model_automatically()\n",
        "    if model_path:\n",
        "        CONFIG['model_path'] = model_path\n",
        "\n",
        "    try:\n",
        "        # Initialize processor\n",
        "        print(\"üöÄ Initializing SegFormer...\")\n",
        "        segformer = VideoSegFormer(\n",
        "            CONFIG['model_path'],\n",
        "            CONFIG['base_model'],\n",
        "            CONFIG['class_names'],\n",
        "            CONFIG['colors']\n",
        "        )\n",
        "\n",
        "        # Open video\n",
        "        cap = cv2.VideoCapture(input_file)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"‚ùå Cannot open video: {input_file}\")\n",
        "            return None\n",
        "\n",
        "        # Get video info\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        print(f\"üìπ Video Info:\")\n",
        "        print(f\"   Resolution: {width}x{height}\")\n",
        "        print(f\"   FPS: {fps:.1f}\")\n",
        "        print(f\"   Frames: {total_frames}\")\n",
        "        print(f\"   Duration: {total_frames/fps:.1f}s\")\n",
        "\n",
        "        # Output file\n",
        "        base_name = os.path.splitext(input_file)[0]\n",
        "        output_file = f\"{base_name}_traffic_segmented.mp4\"\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_file, fourcc, CONFIG['fps_output'], (width, height))\n",
        "\n",
        "        print(f\"\\nüîÑ Processing video for traffic segmentation...\")\n",
        "\n",
        "        # Process frames\n",
        "        frame_count = 0\n",
        "        success_count = 0\n",
        "\n",
        "        with tqdm(total=total_frames, desc=\"Segmenting frames\") as pbar:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # Segment frame\n",
        "                    prediction = segformer.segment_frame(frame)\n",
        "\n",
        "                    # Create overlay\n",
        "                    overlay = segformer.create_overlay(frame, prediction)\n",
        "\n",
        "                    # Add legend\n",
        "                    final_frame = segformer.add_legend(overlay, prediction)\n",
        "\n",
        "                    # Write frame\n",
        "                    out.write(final_frame)\n",
        "                    success_count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Error in frame {frame_count}: {e}\")\n",
        "                    out.write(frame)  # Use original frame\n",
        "\n",
        "                frame_count += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Update progress every 30 frames\n",
        "                if frame_count % 30 == 0:\n",
        "                    pbar.set_description(f\"Segmenting frames - {success_count/frame_count*100:.1f}% success\")\n",
        "\n",
        "        # Cleanup\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        print(f\"‚úÖ Processing complete!\")\n",
        "        print(f\"   Success rate: {success_count/total_frames*100:.1f}%\")\n",
        "        print(f\"   Output: {output_file}\")\n",
        "\n",
        "        return output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Processing failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def show_sample_frames(video_file, num_samples=3):\n",
        "    \"\"\"Show sample frames from processed video\"\"\"\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_file)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Get sample frames\n",
        "        frames = []\n",
        "        indices = [total_frames//4, total_frames//2, 3*total_frames//4]\n",
        "\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if frames:\n",
        "            fig, axes = plt.subplots(1, len(frames), figsize=(15, 5))\n",
        "            if len(frames) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for i, frame in enumerate(frames):\n",
        "                axes[i].imshow(frame)\n",
        "                axes[i].set_title(f\"Frame {indices[i]} - Traffic Segmentation\")\n",
        "                axes[i].axis('off')\n",
        "\n",
        "            plt.suptitle(\"üöó Sample Traffic Segmented Frames\", fontsize=16)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Cannot show samples: {e}\")\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED INTERFACE\n",
        "# =============================================================================\n",
        "\n",
        "def upload_and_segment_traffic():\n",
        "    \"\"\"Simple function to upload and segment traffic video\"\"\"\n",
        "    print(\"üöó UPLOAD AND SEGMENT TRAFFIC VIDEO\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Auto-find model\n",
        "    model_path = find_model_automatically()\n",
        "\n",
        "    if model_path:\n",
        "        print(f\"‚úÖ Using trained model: {model_path}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No trained model found, using base pretrained model\")\n",
        "        print(\"üí° The model will still work but might not be optimal for your specific traffic dataset\")\n",
        "\n",
        "    print(f\"üéØ Traffic classes: {', '.join(CONFIG['class_names'])}\")\n",
        "\n",
        "    # Upload\n",
        "    print(\"\\nüìÅ Select your video file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No file uploaded!\")\n",
        "        return\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"‚úÖ Processing {filename} for traffic segmentation...\")\n",
        "\n",
        "    # Process\n",
        "    output_file = process_video(filename)\n",
        "\n",
        "    if output_file:\n",
        "        print(f\"\\nüéâ Complete! Traffic segmentation finished!\")\n",
        "        print(f\"üìä File size: {os.path.getsize(output_file)/1024/1024:.1f} MB\")\n",
        "\n",
        "        # Show sample frames\n",
        "        show_sample_frames(output_file)\n",
        "\n",
        "        print(f\"\\n‚¨áÔ∏è Downloading {output_file}...\")\n",
        "        files.download(output_file)\n",
        "    else:\n",
        "        print(\"‚ùå Processing failed!\")\n",
        "\n",
        "# =============================================================================\n",
        "# STATUS CHECK AND AUTO-START\n",
        "# =============================================================================\n",
        "\n",
        "def check_setup():\n",
        "    \"\"\"Check if everything is ready\"\"\"\n",
        "    print(\"‚öôÔ∏è SETUP STATUS CHECK\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # Check model and auto-detect classes\n",
        "    model_path = find_model_automatically()\n",
        "    if model_path:\n",
        "        print(f\"‚úÖ Model: {model_path}\")\n",
        "\n",
        "        # Try to detect actual number of classes\n",
        "        try:\n",
        "            state_dict = torch.load(model_path, map_location='cpu')\n",
        "            classifier_key = None\n",
        "            for key in state_dict.keys():\n",
        "                if 'classifier.weight' in key:\n",
        "                    classifier_key = key\n",
        "                    break\n",
        "\n",
        "            if classifier_key:\n",
        "                actual_num_classes = state_dict[classifier_key].shape[0]\n",
        "                print(f\"üìä Detected classes: {actual_num_classes}\")\n",
        "\n",
        "                if actual_num_classes == 6:\n",
        "                    print(\"üîÑ Model trained with 6 classes (likely without background)\")\n",
        "                    adjusted_classes = CONFIG['class_names'][1:]  # Remove background\n",
        "                    print(f\"üìù Adjusted class list: {adjusted_classes}\")\n",
        "                elif actual_num_classes == 7:\n",
        "                    print(f\"üìù Model classes: {CONFIG['class_names']}\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  Custom class count: {actual_num_classes}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not detect classes: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Model: Using base pretrained model\")\n",
        "\n",
        "    # Check device\n",
        "    device = 'GPU' if torch.cuda.is_available() else 'CPU'\n",
        "    print(f\"‚úÖ Device: {device}\")\n",
        "\n",
        "    print(\"\\nüöÄ Ready to process traffic videos!\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "# Run setup check\n",
        "check_setup()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöó HOW TO USE FOR TRAFFIC SEGMENTATION:\")\n",
        "print(\"=\"*60)\n",
        "print(\">>> upload_and_segment_traffic()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Auto-start\n",
        "print(\"\\nüéØ Starting traffic video segmentation interface...\")\n",
        "upload_and_segment_traffic()"
      ],
      "metadata": {
        "id": "lwKLR7wxJ_ss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}